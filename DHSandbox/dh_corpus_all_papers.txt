the gates of hell: history and
definition of digital | humanities |
1. a metaphor
in 1879, edmund terquet (1836-1914), the french secretary of state for fine arts, commissioned a monumental door from the sculptor auguste rodin (1840-1917). rodin's door would be used as the entrance to the planned decorative arts museum in paris. the artist was given three years to complete it, but the museum project started to go wrong, and the state cancelled it in 1889. in the meantime, the door had lost its original setting and function and rodin, freed from the restrictions of designing a functional piece of art, explored the creative possibilities of the surface and created a sculpture which he would constantly revisit until his death. the sculpture, which is on exhibition at the rodin museum in paris,2 is unmistakingly a door, with its two leaves, sideparts, and tympanum. and yet, the door doesn't open. there is no opening mechanism and, even if there were one, the more than 200 figures and groups on the door are too entangled and prevent any movement of the leaves. rodin called his sculpture la porte de l'enfer or the gates of hell, since his original inspiration was the then very popular theme of dante's la divina commedia.
when i was watching the documentary 'a season in hell. rodin's gate',3 it struck me that the story of rodin's sculpture could be used as a metaphor for the field of humanities computing.4 by 'humanities
1 this essay is for ron van den branden: sine te...
2 http://www.musee-rodin.fr/fr/collections/sculptures/la-porte-de-lenfer [accessed 12 january 2013].
3 http://www.canal-educatif.fr/en/videos/art/2/rodin/gates-of-hell.html [accessed 12 january 2013].
4 throughout this essay, humanities computing with capitalization refers to the field and humanities computing without capitalization refers to the activity of computing in and
                           
        120
   computing' i mean the practice of using computing for and in the humanities5 from the early 1950s to 2004 when 'digital humanities' became the prominent name for the field.
just as rodin's 'door', humanities computing consisted of two clearly separated leaves with their own history and understanding behind them but, when put together, they became so heavily interlinked that they could not be separated without any loss of meaning. humanities computing was neither a traditional humanities nor a computing subject. that's why, in the course of time, the self-reflective question what constitutes and defines humanities computing has in itself become a research theme.
however, the main reason why rodin's gates of hell is such a good metaphor for humanities computing is that it is the creative result of failure. the failure on the part of the french government to build the decorative arts museum in paris freed rodin's design from the functional restrictions, and paved the way for an almost exuberant creation. likewise, humanities computing is the creative result of failure on the part of the manufacturers of early computers to produce operational machines in time to be used during the second world war (or, one can argue, of failure on the part of the allied forces to make the war last longer).
2. failure
probably the first mention of the application of computing to the arts is found in the notes to the translation of luigi federico menabrea's (1809-1896) notions sur la machine analytique de charles babbage (1842)6 translated in 1943 as sketch of the analytical engine invented by charles babbage7 (menabrea, 1961 [1843]; lovelace, 1961 [1843]) by augusta ada, countess of lovelace (1815-1852).8 with the poet lord byron (1788-1824) as her father and the mathematician anabella milbanke (1792-1860) as her mother, ada lovelace, as she is more frequently called, may well be considered the personification of the humanities computing educational idea. meditating upon the possible uses
for the humanities.
5 by computing for the humanities, i mean the instrumental use of computing for the
sake of the humanities. by computing in the humanities, i mean the meaning-generating activity of humanities computing.
6 originally published in the bibliotheque universelle de geneve, 82 (october 1842).
7 these notes were published separately in scientific memoirs, selections from the
transactions of foreign academies and learned societies and from foreign journals, edited by richard taylor in 1843.
8 see toole (1996 and 1998) for biographical notes and comments on her work.
                        
121
   of babbage's analytical engine9 for non-numerical purposes, she wrote that the operating mechanism:
might act upon other things besides number, were objects found whose mutual fundamental relations could be expressed by those of the abstract science of operations, and which should be also susceptible of adaptations to the action of the operating notation and mechanism of the engine. supposing, for instance, that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent. (lovelace, 1961 [1843], pp. 248-9)
however, the first computer music wasn't produced before csirac,10 australia's first digital computer, was used to perform the colonel bogey march in 1950 or 1951, and electronic computer music boomed from 1957 onwards with the release of the first program for sound generation, appropriately called music.11 moreover, the submission of musicologist papers to the journal computers and the humanities in the 1960s and 1970s is substantial. but lovelace was right in her observation that computing techniques and devices could have their use in non-numerical applications as well. this was especially realized after the end of the second world war.
in 1943 the us military12 commissioned the building of the eniac (electronic numerical integrator and computer) to calculate trajectories of world war ii artillery guns, a task that involved repetitive sequences of operations on complex mathematical data. the two leading architects of this giant electronic digital calculator were j. presper eckert (1919-1995)13 and john mauchly (1907-1980)14 of the university of pennsylvania's moore school of electrical engineering. before the eniac, these operations had been executed with the use of differential analysers, desk calculators, and punched-card installations, consisting of several serialized punched-card machines (polachek, 1997), a market dominated by ibm at that time.
9 babbage's analytical engine was a proposed programmable mechanical calculator with a planned memory of 1,000 numbers of 50 digits. it used punched cards for the input of instructions, the input and output of data, and the storage of data and instructions.
10 csirac: council for scientific and industrial research automatic computer.
11 the program was written by vernon matthews (b. 1926) at bell labs (talk,
2004 and 2005).
12 more particularly, the ordnance department.
13 see eckstein (1996) for an account of the early life of eckert, and wilkes (1995) for
a tribute to his work.
14 see stern (1980) for a biographical note on mauchly, and mauchly (1984) for an account of his crucial early years of experimenting and research.
                        
        122
   when eniac was assembled and delivered to the us army in 1946, its development and production time had exceeded the war and its envisioned purpose for warfare had therefore become redundant. the same happened with the edvac (electronic discrete variable automatic computer), the first binary stored program computer, which was commissioned from the same team in 1944 and which only became fully operational in 1951.
with the end of the second world war, the urgent need for computing power for warfare purposes disappeared, although the cold war kept the importance of (classified) computer research programs at the top of the intelligence agenda till the early 1990s. among the early thinkers on the social function of computing technology was warren weaver (1894-1978)15 who had been involved with ballistics during the war and who had become director of the natural sciences division of the rockefeller foundation afterwards. inspired by pioneering pre-war computing projects and the developments he witnessed during the war, he started to wonder what sort of applications 'this incredibly powerful tool, the electronic computer' (weaver, 1970, p. 105) could be used for. the warfare computing practices of ballistics and cryptanalysis convinced him that the computer could be used for two peaceful academic applications in particular: one in the sciences and one in the humanities, namely mathematics and machine translation respectively (weaver, 1970, pp. 104-08).
just as the failure of the paris museum project freed rodin from the restrictions of a functional door, the end of the second world war freed weaver from seeing the computer only as a warfare tool for ballistics and cryptanalysis.
3. machine translation
machine translation (mt) is 'the application of computers to the translation of texts from one natural language into another' (hutchins, 1986, p. 15). the arguments for research into mt are pragmatic and social (people have to read documents and communicate in languages they do not know), academic and political (international cooperation and globalization through the removal of language barriers in order to promote peace and further knowledge in developing countries), military (to find out what the enemy knows), scholarly (to study the basic mechanisms of language and mind and exploit the possibilities and limits of the computer), and economical (to sell a successful product).
15 see weaver (1970) for his autobiography, and hutchins (2000b) for a brief biographical note.
                        
123
   the early 1950s saw some experiments with word-for-word translations of scientific abstracts by richard h. richens (1918-1984)16 and andrew d. booth (1918-2009)17 using punched cards (richens and booth, 1952).18 until then, the problem of automating translation was thought of in mechanical terms solely: the development of a dictionary lookup system in aid of the human translator. andrew booth, a crystallographer at birkbeck college (university of london) was probably the first person to refer to the possible use of electronic computers for machine translation. in a memorandum to the rockefeller foundation dated 12 february 1948, he wrote:
a concluding example, of possible application of the electronic computer, is that of translating from one language into another. we have considered this problem in some detail, and it transpires that a machine of the type envisaged could perform this function without any modi cation in its design. (quoted from weaver, 1965 [1949], p. 19)
a rockefeller research fellow at the institute for advanced study at princeton, booth was reporting to warren weaver who, as early as 1946, had had several conversations with booth on the use of automatic digital computers for mechanical translation from one language into another (booth and locke, 1965 [1955], p. 2; booth, 1980, p. 553; hutchins, 1986, p. 24). in the course of his fellowship, booth, together with his assistant kathleen britten (b. 1922), who later became his wife, developed a detailed code for storing a dictionary in an automatic digital computer's memory to be retrieved from standard teletype input. this idea dated back from 1946 and realized dictionary translation on an automatic computer (booth, 1958, pp. 92-9).
it was booth's work and his own experience as a cryptanalyst during the war that formed the basis for weaver's memorandum 'translation' which was issued on 15 july 1949 (weaver, 1965 [1949]). the weaver memorandum was circulated amongst twenty or thirty 'students of linguistics, logicians, and mathematicians' (weaver, 1970, p. 107) and up to 200 scholars (locke and booth, 1965 [1955], p. 15) in different fields. it was this memorandum which initiated research projects at different
16 see sparck jones (2000) for an overview of richens' work in mt.
17 see booth (1997) and booth and booth (2000) for an overview of booth's work in
mt. see booth (1980) for details on his work in crystallography and the development of early computers and magnetic storage devices. booth was also the holder of the british patent on the floppy disk.
18 this paper 'some methods of mechanized translation' was written in 1948, but not published before 1955 (sparck jones, 2000, p. 263). the paper was presented on the first conference on mechanical translation at mit in june 1952.
                        
        124
   universities and generated some early writings on the problems involved with machine translation.19 these problems included ambiguity of words, the semantic function of syntax, and the resolution of word order problems in different languages.
in 1952, eighteen scholars, including booth as the only non-american delegate, gathered on the first 'international' conference on machine translation at mit, followed by a meeting later that year in london where some forty linguists met during the international linguistic congress. a year later, machine translation appeared for the first time in a scholarly textbook written by andrew and kathleen booth. in their book automatic digital calculators (booth and booth, 1953), aimed at a readership of computer scientists, the authors published a chapter on 'some applications of computing machines' in which machine translation was discussed at length.20 in 1954, a widely publicized demonstration took place at ibm headquarters and involved a carefully selected sample of 49 russian sentences, a limited vocabulary of 250 russian words from different fields and their english equivalents, and six rules of syntax. the ibm press release quoted: 'a girl who didn't understand a word of the language of the soviets punched out the russian messages on ibm cards. the 'brain' dashed off its english translations on an automatic printer at the breakneck speed of two and a half lines per second.'21 in the same year, the first doctoral dissertation on machine translation was presented by anthony oettinger (b. 1929) at harvard university (oettinger, 1954) and the journal mechanical translation appeared for the first time.
from 1955 to 1966, the field organized itself in groups working mainly on dictionary, lexicographic, and semantic problems and groups working on syntactic problems; in groups that took an empirical approach (mainly in the uk) and others that took a theoretical approach (mainly in the us); and in groups working towards operational systems in the short term, and groups working toward high quality systems in the long term. these years saw a dozen important conferences, gatherings, and sessions on machine translation, and the founding of the association for machine translation and computational linguistics (amtcl) on 13 june 1962. apart from the us and the uk, research was undertaken in e.g. bulgaria, canada, former czechoslovakia, france, israel, japan, the former ussr, and the later independent states.22
19 see the annotated bibliography in locke and booth (1965 [1955], pp. 227-36).
20 the book was reprinted twice, in 1956 and 1965, and translated into russian in
1957.
21 ibm press release, 8 january 1954, http://www-03.ibm.com/ibm/history/ exhibits/701/701_translator.html [accessed 12 january 2013].
22 for recollections and overviews of research in these countries, see the different papers in hutchins (2000a).
                        
125
   whereas the funding agencies in the us had applauded the importance of machine translation to 'the overall intelligence and scientific effort of our nation' in a report compiled by the committee on science and astronautics of the us house of representatives in june 1960 (hutchins, 1986, p. 159), six years later the  same conclusions of an independent advisory committee installed by request of the funding bodies put an end to the funding of research in machine translation in the us. the notorious alpac23 report 'languages and machines: computers in translation and linguistics' (alpac, 1966) criticized the need, cost, and performance of automatic translations and even suggested that, since english is the dominant language in science, it was more cost efficient to teach heavy users of translated russian articles russian than to provide them with a translation service. the final recommendations outlined that funding should be provided for the improvement of translation by developing machine aids for human translators and for computational linguistics, which had grown out of machine translation. the alpac report put the research towards perfect translation to an end and referred its ideal to the realm of utopia.24 for linguistics in general, and for computational linguistics and humanities computing in particular, the report put the future research programme on language in a different perspective, or as victor yngve put it:
the future of linguistics is not in philosophy, from which it is emerging, but in standard science, into which it can now move with confidence . this requires that linguistics finally recognize that the true object of study of a scientific linguist is the people that speak and understand and communicate in other ways, and other relevant aspects of the real world. (yngve, 2000, p. 69)
roberto busa (1913-2011) seemed to agree with yngve when he identified  the major problem with research in machine translation not as the inadequacy of computers to deal with human language, but as man's insufficient  comprehension of human languages (busa, 1980, p. 86).
23 automatic language processing advisory committee.
24 the alpac report caused ten years of neglect of machine translation from the
scientific world, and more from the funding bodies, and it fostered a general belief with the public that machine translation was more fiction than science. a renewed interest in machine translation can be observed from the 1980s onwards with a new journal, computers and translation (1986), which changed its name to machine translation (1989), and a series of international conferences and summits. in 1989, literary and linguistic computing (4 (3)) devoted a special section to machine translation containing six papers edited and introduced by antonio zampolli.
                        
        126
   4. lexical text analysis
machine translation was highly involved with the electronic processing of humanities data. early writings on machine translation mention the essential use of concordances, frequency lists, and lemmatization, which were, according to antonio zampolli (1937-2003) typical products of lexical text analysis (lta) (zampolli, 1989). in this respect, it is not surprising to find an article on 'the computer in literary studies' in a volume devoted to machine translation (booth, 1967).
collaboration between scholars of machine translation and of lexical text analysis in the 1950s and early 1960s has been reported by michael levison, who joined booth's laboratory as a phd student in 1958. although booth's humanities-based work was mainly situated in machine translation, there was a strong interest in the application of the computer to other linguistic processes from the mid-1950s onwards (booth et al., 1958). programs for the statistical analysis of text, stylometry, and the production of concordances were developed in the early 1960s (levison, 1962) and in his 1967 article on 'the computer in literary studies' levison describes the following classes of literary problems in which computers can be used successfully: concordances, glossaries, authorship attribution, stylistic studies, relative chronology, fragment problems with papyri, and even a preliminary form of the digital library described as a tape library (levison, 1967). a 'steady stream of visitors' who came 'seeking help with literary and linguistic problems' (lessard and levison, 1998, p. 262) frequented booth's laboratory to work on all of these literary problems, and even a couple of geographers turned up with a proposal to investigate the possibility of accounting for 'polynesian settlement by drift voyaging', using simulation (lessard and levison, 1998, p. 262).
although booth had left the laboratory before all of these projects came to fruition, it is certainly his inspiration and reputation that brought about the cooperative ventures. two of booth's students, leonard brandwood and john cleave, may even have been the first phd students who applied computers to non-translation language problems in the humanities. brandwood worked on the chronology and concordance of plato's works (booth et al., 1958, pp. 50-65), and cleave on the mechanical transcription of braille (booth et al., 1958, pp. 97-109).
one of the most important early computing projects which made use of lexical text analysis, however, was roberto busa's index thomisticus, a lemmatized concordance of all the words in the complete works of thomas aquinas. although the first mention of the project was a short project description published in speculum in january 1950 (busa,
                        
127
   1950),25 busa himself dates his original idea of using modern mechanical techniques for the linguistic analysis of written texts between 1941 or 1942 (busa, 2004b, p. xvi; busa 2002, p. 49) when he started his phd research, and 1946 when he completed his dissertation and was looking for a follow-up research project (busa, 1980, p. 83). the fact is that busa's dissertation (busa, 1949) was written without the use of or reference to any computer technology.26 but in 1951 busa teamed up with people from ibm in new york to automatically compile a concordance of the poetry of thomas aquinas, which was the first example of a word index printed by punched-card machines (busa, 1951). however, this proof of concept exercise used no computing and no programming. the main innovation was busa's insight that commercial accounting machines could be used for humanities purposes with good results. the result of the 1951 project offered six scholarly tools: an alphabetical frequency list of the words; a retrograde frequency list of the words; an alphabetical frequency list of words set out under their lemmata; the lemmata; an index of the words; and a kwic concordance (winter, 1999).
for his complete index thomisticus, busa calculated that the stack of punchcards would have weighed 500 tonnes, occupying 108 m3 with a length of 90 m, a depth of 1 m, and a height of 1.20 m. by 1975, when the index thomisticus was completed and started to appear on 65,000 pages in 56 volumes (busa, 1974-1980) some 10,631,973 tokens were processed.27 this processing consisted of inputting, verifying, and interpreting with references and codes which specify the values within the levels of the morphology the 'internal hypertext' in busa's terminology (busa, 2002 and 2004a). the work was done by a team of keypunch operators who were trained in busa's own training school which ran from 1954 to 1967 (busa, 1980, p. 85).
whereas busa was using keypunch technology in close cooperation with ibm, john w. ellison completed his computerized concordance to the revised standard version of the bible with the computing facilities offered by remington rand, namely magnetic tape technology and the univac i mainframe computer (universal automatic computer)28 in 1957. the story goes that busa met ellison around 1954, congratulated
25 although this publication is mentioned in busa's bibliography published in busa (2002), it is not clear whether busa is really the author of the piece, which is written in the third person.
26 1949 is used as the symbolic start of computational work in the humanities by several authors. cf. recently burdick et al. (2012, p. 123).
27 figure according to the project report opera quae in indicem thomisticum sunt redacta (1975, revised 1980), privately made available to me.
28 the univac computers were built by the same team which built the eniac and edvac computers.
                        
        128
   him on his computing work, and went back to ibm to transfer the punch cards onto magnetic tape and use computer technology and programming29 for the publication of his dead sea scrolls project in 1957.30 for the index thomisticus, busa was working on 1,800 tapes, each one 2,400 feet long, and their combined length was 1,500 km (busa, 2004b, p. xvii).
ellison dates his original idea of using 'modern mechanical devices' back to 1945 when he realized that distinguished scholars 'having two or three earned doctorates, were essentially counting on their fingers  as they studied manuscripts' (ellison, 1965, p. 64). in 1950, he asked for computing time at the harvard computation laboratory, which was granted in 1951. his proof of concept exercise was the internal collation of 309 manuscripts of the st. luke gospel, printed against the standard text with a classification of eight kinds of variant readings with the mark iv computer in 1952 or 1953. this was the first example of a manuscript collation carried out and printed by a computer.
up to the publication of the infamous alpac report in 1966, computational linguistics and lexical text analysis were not separated fields, and used statistical analysis for the creation of indexes, concordances, corpora, and dictionaries. but from then onwards, computational linguistics embraced the symbolic approach and abandoned statistical analysis which has been at the heart of humanities computing.
5. literary and linguistic computing and computing in/for the humanities
the history of both machine translation and lexical text analysis are closely related to the technological development of computing machinery, program languages, and software and the economic opportunities identified  by their manufacturers. in the years following the end of the second world war, traditional manufacturers and suppliers of analog tabulating equipment changed their core business to digital computing equipment and services, and were prospecting new markets. this is why key players like remington rand and ibm teamed up with humanities scholars and funded conferences and projects that explored new applications of computing. one such early conference was held at yale university in january 1965 under the hesitating title computers for the humanities?
29 since fortran was only released in 1975, the programming was still in card management.
30 also in 1957, and independently of the work of busa or ellison which hadn't appeared yet, cornell university launched a program for a computer-produced series of concordances, with stephen m. parrish as general editor (parrish, 1962, p. 3).
                        
129
   the cover of the proceedings, published under the same title (pierson, 1965), shows a silhouette drawing of rodin's le penseur (the thinker) punched like a punched card to indicate the link between computing and the history of ideas. the proceedings contain papers on the history of computing and the use of computers in the sciences; on computers and words; language and literature; computers and history; computers and the arts; and a discussion of some possibilities and speculations on future computer projects. this book is probably the earliest volume surveying the early use of computing in the humanities beyond machine translation. two years later, the selected papers from six such conferences sponsored by ibm,31 which were attended by some 1,200 academics from all over the us in 1964 and 1965, were published under the not so hesitating title computers in humanistic research. readings and perspectives (bowles, 1967). the papers in this book deal with computational applications in anthropology, archaeology, history, political sciences, language, literature, and musicology.
in the uk, the literary and linguistic computing centre (llcc) at the university of cambridge was set up with roy wisbey (b. 1929) as its first director in 1964. it was also wisbey who organized the first international conference on the use of the computer in literary and linguistic research which brought together british scholars with participants from australia, canada, continental europe and the us in 1970.32 in 1972, a second such conference was organized in edinburgh.33 the emphasis on literary and linguistic computing was also reflected in the name of the association for literary and linguistic computing (allc)34 which he co-founded in 1973 and chaired from 1973 to 1978. the allc published a periodical called allc bulletin from 1973 to 1985 and the allc journal from 1980 to 1985. in 1986 both publications were replaced by the journal literary and linguistic computing (llc) which in 2005 changed its name to llc: the journal of digital scholarship in the humanities. the allc started to organize a series of biannual conferences on literary and linguistic computing under its own name and the two previous conferences were added to the list.35 from 1973 onwards, these conferences alternated with an american series of biannual conferences called international
31 the conferences were held in 1964 and 1965 at rutgers, yale, ucla, the consortium of universities in washington dc, purdue, and boston university.
32 proceedings published in wisbey (1971).
33 proceedings published in aitken et al. (1973).
34 http://www.allc.org [accessed 12 january 2013]. the allc was recently rebranded
as eadh: the european association for digital humanities.
35 this explains why the first two allc conferences listed on the eadh website were
organized before the founding of the association.
                        
        130
   conference on computers in the humanities (icch) in the odd years.36 the professional association which was founded in the us in 1978 was hence called the association for computers and the humanities (ach).37 twelve years before, its founding president joseph raben (b. 1924) had started to edit the journal, computers and the humanities (chum) which ran from 1966 to 2004.38 whereas the scope in europe was mainly on literary and linguistic studies of language in literary form, the american conferences, journal and association showed a broader interest in computer-based studies of language in literary and non-literary form. this is reflected in the titles of the proceedings from the conferences held in the 1970s: the computer in literary and linguistic research (wisbey, 1971); the computer and literary studies (aitken et al., 1973); the computer in literary and linguistic studies (jones and churchhouse, 1976); and advances in computer-aided literary and linguistic research (ager et al., 1979) in europe, and computers in the humanities (mitchell, 1974) and computing in the humanities (lusignan and north, 1977) in north america. from the start, the icch conferences also included papers on history, musicology, computer assisted instruction, and creative arts (dance, music, poetry).39
the first monographs about computers in the humanities, however, came from the computer industry. in 1971, ibm published a series of application manuals on computing in the humanities: introduction to computers in the humanities (ibm, 1971a); literary data processing (ibm, 1971b); and computers in anthropology and archaeology (ibm, 1971c). almost a decade later, and after thirty years of computing in the humanities, supporters on both sides of the atlantic were treated to two textbooks on the topic which appeared in the same week in january 1980. susan hockey's a guide to computer applications in the humanities (hockey, 1980a) and robert oakman's computer methods for literary research (oakman, 1980)40 provided the first consistent overviews from
36 with the exception of allc conferences in 1985, 1986, 1987, and 1988. from 1989 a joint conference was organized yearly in europe in the even and in the us or canada in the odd years. from 1990 to 2005, the conference was called allc/ach or ach/allc depending on the location. with the foundation of adho in 2005, the conference was renamed 'digital humanities'.
37 http://www.ach.org [accessed 12 january 2013].
38 in 1968 another journal was launched: computer studies in the humanities and
verbal behaviour.
39 in the uk, however, a separate series of conferences on computer assisted teaching in the humanities (cath) were organized.
40 notice that it's now hockey who uses 'computers in the humanities' in the title of her book, and oakman narrowed it down to 'literary research'.
                        
131
   an academic point of view.41 although both books filled an urgent need for a surveying textbook in the field of literary and linguistic computing, they were not explicitly conceived with a didactic point of view. the authors brought together the issues raised in the journal papers, the several collected volumes of conference proceedings, the available project reports, and the scarce manuals for specific  programming languages and applications 'from the unifying perspective of one observer' (oakman, 1980, p. x) and were very much alike.42 in synthetizing thirty years of research, the books became reference points for further writing on the history in the field. in this respect it's relevant to notice that hockey identified  busa as the pioneer of humanities computing, whereas oakman named ellison.
one of the first mentions of 'humanities computing' to name the activity of computing in and for the humanities was in an article in the second issue of chum about the use of pl/i as a programming language for humanities research in 1966 (heller and logemann, 1966).43 in 1968 aldo duro published a survey of 'humanities computing activities in italy' (duro, 1968) which suggests that that the term was already well known, though not dominant in the community.44 whereas the late 1960s saw the introduction of the term to name the computing activity, the term began to mark the field in the early 1970s, as we can see in stacey tanner's report on the allc conference of 1974 published in dataweek and reprinted in allc bulletin (tanner, 1975). tanner reported on busa's address to the conference by paraphrasing the term is not used by busa himself that he talked about 'the future of humanities computing' and about 'projecting the programs of humanities computing' (tanner, 1975, p. 54). by the 1980s, the use of the term for the field was widespread, as demonstrated in busa's retrospective paper 'the annals of humanities computing: the index thomisticus' (busa, 1980) although neither hockey (1980a) nor oakman (1980) use the term to name the field. from the mid-1980s onwards, 'humanities computing' started to appear in the
41 although howard-hill's literary concordances (howard-hill, 1979) claimed to be 'a complete handbook for the preparation of manual and computer concordances', the book deals very little with computing. it was, however, published a year before hockey's and oakman's books. hockey had finished  writing her book in 1978, but the publisher sat on the manuscript for quite a while. she heard about oakman writing his book when her manuscript was already at the publishers (susan hockey, personal communication, 5 june 2005). oakman completed his manuscript in early 1978 (oakman, 1984, p. xv) and explicitly mentioned hockey's book in the revised reprint from 1984.
42 'i think the similarities are due to the fact that there wasn't a lot of material to draw on, only the proceedings of some conferences and chum and the allc publications' (susan hockey, personal communication, 5 june 2005).
43 with thanks to willard mccarty for providing me with a copy of this article.
44 the university of colorado already had an operational humanities computing
facility around the same time.
                        
        132
   names of north american teaching programmes (ide, 1987), computing centres (university of washingon and mcmaster university) and facilities (arizona state university, duke university and ucla). although the use of the name to delimit a distinctive and coherent discipline was a frequent matter of debate (miall, 1990, p. 3), the publication of two volumes of the humanities computing yearbook (lancashire and mccarty, 1988 and lancashire, 1991) and five volumes of research in humanities computing (1991-1996) established the name after almost two decades of hesitating use.
6. text encoding
one of the main problems since the earliest uses of computers and computational techniques in the humanities was the representation of data for input, processing, and output. computers, as michael sperbergmcqueen has reminded us, are binary machines that 'can contain and operate on patterns of electronic charges, but they cannot contain numbers, which are abstract mathematical objects not electronic charges, nor texts, which are complex, abstract cultural and linguistic objects' (sperbergmcqueen, 1991, p. 34). this is clearly seen in the mechanics of early input devices such as punched cards where a hole at a certain coordinate actually meant a i or 0 (true or false) for the character or numerical represented by this coordinate according to the specific  character set of the computer used. because different computer systems used different character sets with a different number of characters, texts first had to be transcribed into that proprietary character set. all characters, punctuation marks, diacritics, and significant changes of type style had to be encoded with an inadequate budget of characters. this resulted in a complex set of ' ags' for distinguishing upper-case and lower-case letters, for coding accented characters, the start of a new chapter, paragraph, sentence, or word. these ' ags' were also used for adding analytical information to the text such as word classes, morphological, syntactic, and lexical information. ideally, each project used its own set of conventions consistently throughout. since this set of conventions was usually designed on the basis of an analysis of the textual material to be transcribed to machine-readable text, another corpus of textual material would possibly need another set of conventions. the design of these sets of conventions was also heavily dependent on the nature and infrastructure of the project, such as the hardware and software.
although several projects were able to produce meaningful scholarly results with this internally consistent approach, the particular nature of each set of conventions or encoding scheme had lots of disadvantages. texts
                        
133
   prepared in such a proprietary scheme by one project could not readily be used by other projects; software developed for the analysis of such texts could hence not be used outside the project due to an incompatibility of encoding schemes and non-standardization of hardware. however, with the increase in texts being prepared in machine-readable format, the call for an economic use of resources increased as well. already in 1967, michael kay argued in favour of a 'standard code in which any text received from an outside source can be assumed to be' (kay, 1967, p. 171). ideally, this code would behave as an exchange format which allowed the users to use their own conventions at output and at input (kay, 1967, p. 172).
some sort of standardization of markup for the encoding and analysis of literary texts was reached by the cocoa encoding scheme originally developed for the cocoa program in the 1960s and 1970s (russell, 1967) but used as an input standard by the oxford concordance program (ocp) in the 1980s (hockey, 1980b) and by the textual analysis computing tools (tact) in the 1990s (lancashire et al., 1996). for the transcription and encoding of classical greek texts, the beta-transcription/ encoding system reached some level of standardized use (berkowitz and squiter, 1987).
in 1987, a group of thirty-two humanities scholars45 gathered at vassar college in poughkeepsie, new york in a two-day meeting (11 and 12 november 1987) called for by the ach and convened by nancy ide and michael sperberg-mcqueen. the main topic of the meeting was the question how and whether an encoding standard for machine-readable texts intended for scholarly research should be developed. the conclusions of the meeting were formulated as a set of methodological principles the so-called 'poughkeepsie principles'46 for the preparation of text encoding guidelines for literary, linguistic, and historical research (burnard, 1988, pp. 132-3; ide and sperberg-mcqueen, 1988, pp. e.6-4, and 1995, p. 6).
for the implementation of these principles the ach was joined by the allc and the association for computational linguistics (acl).47 together they established the text encoding initiative (tei) whose mission it was to develop workable text encoding guidelines. the tei very soon came to adopt the standard generalized markup language (sgml), an iso standard published in 1986 (goldfarb, 1990), as the recommended
45 amongst the delegates were representatives from the main european text archives and from important north american academic and commercial research centres.
46 i am quoting the poughkeepsie principles in 'module 0: introduction' of tei by example, http://www.teibyexample.org on which this section on text encoding is based [accessed 12 january 2013].
47 http://www.aclweb.org [accessed 12 january 2013].
                        
        134
   encoding format for electronic texts.48 michael sperberg-mcqueen was appointed editor-in-chief and lou burnard as european editor of the guidelines.
the first public proposal for the tei guidelines was published in july 1990 under the title guidelines for the encoding and interchange of machine readable texts with the tei document number tei p1 for proposal 1 (sperberg-mcqueen and burnard, 1990). further development of the tei guidelines was done by four working committees (text documentation, text representation, text analysis and interpretation, metalanguage and syntax) and a number of specialist working groups.49 the results of that work included substantial amounts of new material and were published chapter by chapter as tei p2 between march 1992 and the end of 1993 (sperberg-mcqueen and burnard, 1992-1993).
in 1999, the initial development work was concluded with the publication of a 1,292-page documentation of the definitive guidelines as the tei p3 guidelines for electronic text encoding and interchange (sperberg-mcqueen and burnard, 1999), defining some 439 elements. with this work, the poughkeepsie principles were met by providing a framework for the encoding of texts in any natural language, of any date, in any literary genre or text type, without restriction on form or content and treating both continuous materials ('running text') and discontinuous materials such as dictionaries and linguistic corpora.
the advent and the success of the extensible markup language (xml)50 as an industry standard replacing sgml from 1999 onwards called for an xml-compatible edition51 of the guidelines (sperberg-mcqueen and burnard, 2002), published in 2002 by the newly formed tei consortium.52
48 initial funding was provided by the us national endowment for the humanities, directorate general xiii of the commission of the european communities, the canadian social science and humanities research council, and the andrew w. mellon foundation.
49 amongst which were groups on character sets, textual criticism, hypertext and hypermedia, formulae, tables, figures, and graphics, language corpora, manuscripts and codicology, verse, drama and performance texts, literary prose, linguistic description, spoken text, literary studies, historical studies, print dictionaries, machine lexica, and terminological data.
50 http://www.w3.org/tr/rec-xml [accessed 12 january 2013].
51 the xml support was realized by the expression of the tei guidelines in xml
and the conformation to a tei conformant xml dtd. the tei consortium generated a set of dtd fragments that can be combined together to form either sgml or xml dtds and thus achieved backwards compatibility with tei p3 encoded texts. in other words, any document conforming to the tei p3 sgml dtd was guaranteed to conform to the tei p4 xml version of it. this 'double awareness' of the tei p4 is the reason why this version was called an 'xml-compatible edition' rather than an 'xml edition'.
52 the tei consortium was established in 2000 as a not-for-pro t membership organization to sustain and develop the text encoding initiative (tei), http://www.tei-c.org [accessed 12 january 2013].
                        
135
   with this xml-compatible version of the p4 guidelines, equal support was provided for xml and sgml applications using the tei scheme, while ensuring that documents produced to earlier tei specific ations remained usable with the new version.
in 2003 the tei consortium asked their membership to convene special interest groups (sigs) whose aim could be to advise revision of certain chapters of the guidelines and suggest changes and improvements in view of a new p5 version. with the establishment of the new tei council, which superintends the technical work of the tei consortium, it became possible to agree on an agenda to enhance and modify the guidelines more fundamentally, which resulted in a full revision of the guidelines published as tei p5 (tei consortium, 2007). tei p5 contains a full xml expression of the tei guidelines and introduces new elements, revises content models, and reorganizes elements in a modular class system that facilitates  exible adaptations to users' needs. contrary to its predecessor, tei p5 does not offer backwards compatibility with previous versions of the tei.53
7. back to the metaphor
curiously, rodin's la porte de l'enfer is called the gates of hell in english and not the 'gate' of hell as the french would suggest. but actually it's a better translation. not only are there many interpretations possible of rodin's vision of hell, there is also more than one sculpture with that name. when rodin failed to enter his sculpture for the 1900 universal exposition in paris, he set up an independent exhibition with the painter claude monet (1840-1926). for this exhibition, he created a plaster model54 of the sculpture from which most figures and groups are deliberately stripped, leaving nothing but undulations in the surface with no clear focal points. by doing this, he moved from narration to expression. since the bronze door was state property, and was thus unsellable, he regrouped, enlarged, and cast some of the stripped figures and groups as separate, marketable works, such as the thinker, the kiss, fleeting love, and the three shades.55 others were reworked in different materials and sizes, such as crouching woman. the abstract character of
53 the tei consortium has, however, maintained and corrected errors in the p4 guidelines for five more years, up to the end of 2012. since that date, the tei consortium has ceased official  support for tei p4, and deprecated it in favour of tei p5.
54 on display at the rodin museum, in meudon.
55 by that time, rodin needed to make a living by selling the individual sculptures.
                        
        136
   the plaster representation of the work also created a renewed interest in his work.
also in this respect, the gates of hell is an appropriate metaphor for humanities computing and digital humanities. over the course of time, humanities computing struggled with problems of self-representation and marketing itself as a discipline or a field. finding a common practice, theoretical principles, methodology, or philosophy across humanities disciplines which employed computational techniques didn't seem to be that straightforward. the doubts about the validity of a distinctive and coherent discipline which were raised in the mid-1980s still remain 'while most of what is called 'humanities computing' is carried out within specific  humanities subjects' (miall, 1990, p. 3). as an applied method, humanities computing sold itself as an archipelago (mccarty, 2006; chapter 5 in this volume) of humanities disciplines, as demonstrated in disciplinary organized teaching programmes, chapters in collected volumes, and strands on conferences. the (hi)story of humanities computing has long been the (hi)story of specific  subjects, such as authorship studies, electronic textual editing, narratology, and multimedia studies or of the use of computing in broader fields such as history, musicology, lexicography, or performing arts.
the isolation of these subjects paved the way for humanities computing to rebrand itself with the more non-jargon-like but more abstract term 'digital humanities', which generated a new interest in the field, especially from the broader audience. the hermetic activity of humanities computing was replaced by a convenient hipster qualification of the humanities. the real problems of self-representation and definition, however, remained the same.
8. self-representation
if we know what it is that we do in humanities computing or digital humanities, the argument goes, we should be able to communicate about that research for the purpose of identifying our work, gaining acknowledgement and academic kudos, and furthering research through (interdisciplinary) collaboration and the development of advanced strategies and tools. as melissa terras warned us in her opening keynote address to interface 2011, 'we should be careful what view of ourselves we are projecting into the wider academic world' (terras, 2011a). in her dh2010 closing plenary 'present, not voting: digital humanities in the panopticon' (terras, 2011b; chapter 18 in this volume) terras threw at us how bad we are at representing ourselves as a field and as a community. towards the end of her lecture, she presented an agenda for the digital
                        
137
   identity, impact, and sustainability of the field. central to this agenda is the development of a definition of the field, the articulation of the field's relevance, success, and impact, the historical knowledge of the field as a discipline, and the preservation of the discipline's heritage.
willard mccarty agrees with terras that historical knowledge about and definition of the field are central issues for the awareness and selfrepresentation of the field. 'a genuine history of the digital humanities in its first half-century', mccarty argued recently, 'would greatly help us turn pitiful laments and dull facts into the stimulating questions we should be asking now' (mccarty, 2012). more challenging than writing a historiography of humanities computing based on existing chronologies is the writing of an historical account for which the historian 'would have to locate practitioners' minority concerns within the broad cultural landscape of the time and then describe the complex pattern of confluence and divergence of numerous interrelated developments' (mccarty, 2012).
however, self-representation has long been restricted to the presentation of chronological overviews and surveys of the field. mccarty himself, for example, in explaining the title of his seminal book humanities computing, writes that it 'names a field of study and practice found both inside and beyond the academy in several parts of the world' (mccarty, 2005, p. 2). as an illustration he refers to a much longer description that is established in his and matthew kirschenbaum's 'institutional models for humanities computing' (mccarty and kirschenbaum, 2003), a structured list of 'departments, centres, institutes and other institutional forms that variously instantiate humanities computing' (mccarty and kirschenbaum, 2003, p. 465).56
9. humanities computing
this re ex to refer to a list of instantiations of what is covered by the name, and thus to provide enumerative descriptions rather than definitions, is typical for attempts to define fields of (scholarly) activities. etymologically, definition comes from the latin 'definitio' which literally means demarcation or fencing. a definition therefore formally freezes the meaning of a term and since humanities computing as a field of activity was in constant flux, a formal description was therefore impossible. this
56 apart from the fact that the url in the book is broken as a result of the allc website's redesign, the list has been superseded by the information on digital humanities centres published by centernet, http://digitalhumanities.org/centernet [accessed 12 january 2013]. see also dan cohen's list of digital humanities scholars and centres on twitter, https:// twitter.com/#!/dancohen/digitalhumanities/members [accessed 12 january 2013].
                        
        138
   impossibility has thus been bypassed by providing enumerations of these activities, as in chronologies or overviews of the field. in the late 1980s and early 1990s such overviews have been provided, for instance for history and computing (adman, 1987), for computing in musicology from 1966 to 1991 (hewlett and selfridge-field, 1991), and for publications in chum on statistical analysis of literature between 1966 and 1990 (potter, 1991). more recently a companion to digital humanities (schreibman et al., 2004) published such surveys of archaeology, art history, classics, history, lexicography, linguistics, literary studies, music, multimedia, performing arts, and philosophy and religion, albeit mainly from an anglo-american point of view.
more general surveys of developments in humanities computing have reflected on the community's activities for a specific  purpose and are either addressed to the community itself or intended for a broader audience. in 1987, susan hockey briefly  discussed the availability of hardware, software, textual data, and courses for the humanist, and the acceptance of computational techniques in the humanities in an assessment of the significant impact that humanities computing developments had on teaching (hockey, 1987).57 three years later, hockey called for a critical appraisal of the activities in the field and she advised a shift of the metacritical emphasis from methodology to modelling in her conclusion to a chronological survey of the available and emerging tools since the early 1960s (hockey, 1990). ian lancashire used his reflections on the activities in literary and linguistic computing in the period from 1968 to 1988 to develop strategies for the future. as a way of rethinking the purpose of the present by a reflection on the past, he promoted both the transformation of research into teaching, and the study of meaning as an important agenda for the future of humanities computing (lancashire, 1990). as a last example i mention here the report on computers and the humanities published by the european science foundation in 1992 (genet and zampolli, 1992) which was conceived as an introduction for the research communities and policy makers in the humanities, the social, and natural sciences to the challenges and the potential of the transversal and interdisciplinary characteristics of computer-based humanities research. this book, submitted as a memorandum to the standing committee for the humanities of the esf, covers the state of the art of computing methods in humanistic research, and presents overviews of journals, institutions,
57 especially the rapid development of the independently operable 'microcomputers' as opposed to the 'mainframes' and the rapid increase of storage capacity revolutionized the way in which humanities computing interacted with teaching in higher education, e.g. with respect to searching through large collections of texts.
                        
139
   and projects in humanities computing, next to critical reflections on the development and future agenda of the field.58
apart from the fact that such endeavours seem to approach humanities computing as a semantic primitive, these more or less chronological surveys all honour the implicit premise that historical knowledge about a field provides that field with its definition or at least enables the detection of theoretical, methodological, and philosophical commons which are both formative and indicative to the field under study. they mainly concentrate on performance and, apart from the concluding research agendas, avoid any involvement with predictions. in other words, they don't provide a definition of 'humanities computing', only surveys of activities and tools in the field.59
one could argue of course that by studying what is being done in the field, we may better understand it (warwick et al., 2012, p. xiii) and that chronologies provide humanities computing with a definition in use or a contextual definition by chronologically reporting on its activities. the problem here is the chronology of the definition. from its definition it follows that a contextual definition may not contain the expression that is de ned but must use an equivalent not containing that expression (oed online, definition, 4.c). in recounting the chronology of humanities computing, this equivalent is a virtual contraction consisting of all given contextual definitions or descriptions of the activities which are, at the moment of defining, considered as belonging to the field. mccarty's reference to the list of institutional models at the beginning of his book (2005) thus defines all elements of that list more in terms of humanities computing than it defines humanities computing in terms of its (alleged) activities.
however, even if a genuine history of the field existed, it would still need to be complemented with methodological awareness, as mccarty argues in humanities computing (mccarty, 2005). methodology is at the basis of any transfer of knowledge about computing in the humanities, which is where terras and mccarty locate the problem for a fruitful debate about the interdisciplinarity of the field. this is, however, not a recent problem, since raben already pointed out in 1973 that the funding bodies' general ignorance of the methods of computing in the humanities was the greatest hindrance to its development and success: 'in their eyes, the preparation
58 the cited overviews were an attempt to scope out the activities at a time when the field was still surveyable and when networked infrastructure and resources were inexistent.
59 in these chronologies, it's the chronologist's intuition, not a definition of the field, which determines the scope and focus of the inventory. they argue falsely that pointing at the field provides it with a definition.
                        
        140
   of a text seems like secretarial work, but the publication of a book comes within the definition of scholarship' (raben, 1973, p. 5).
with his book, mccarty has tried to fill in this knowledge vacuum by attempting to 'anatomize the method of humanities computing into four perspectives: analysis, synthesis, context and profession' (mccarty, 2005, p. 6). analysis and synthesis are the conventional methods of all humanities disciplines, the first of which is the realm of the private scholar, whereas the second is essential to the sociological role of the scholar in the academy and preferably also in the outside world. since humanities computing is in the humanities, as mccarty (2005) sufficiently  argues, its general method does not differ from those of the conventional humanities disciplines. the computational aspect offers the humanities scholar the opportunity to develop alternative analytical approaches towards the subject matter. the difference between computing for the humanities (instrumental) and computing in the humanities (methodological) is exactly the lack (in the former case) or the importance (in the latter case) of modelling as the most essential analytical method of the many forms of computing. whereas the latter is the realm of humanities computing, both exist side by side in digital humanities.
by modelling, mccarty means the 'heuristic process of constructing and manipulating models'; a model, mccarty takes to be either 'a representation of something for purposes of study' (denotative model) or 'a design for realising something new' (exemplary model) (mccarty, 2003b; 2004, p. 255; 2005, p. 24). the purpose of modelling is never to establish the truth directly, but it 'is to achieve failure so as to raise and point the question of how we know what we know' (mccarty, 1999b), 'what we do not know,' and 'to give us what we do not yet have' (mccarty 2004, p. 255). humanities computing shares this methodological characteristic with, for instance, computer science, but reverses the model. humanities computing starts from the modelling of 'imperfectly articulated knowledge' (mccarty, 2005, p. 194), and works its way up through further steps of computational modelling till it reaches the stage of a deeper understanding of the world. computer science, and programming in particular, starts from a real world problem and travels down to its implementation in hardware.60
the method shared by the humanities and computer science that manfred thaller and tito orlandi argued for in their respective defences of a 'humanistic computer science' (thaller, 2001 and 2006)61 or
60 see mccarty's 'stages of modelling' (mccarty, 2005, p. 197).
61 thaller is professor of historisch-kulturwissenschaftliche informationsverarbeitung
which he translates as 'computer science for the humanities' on his website http://www.hki. uni-koeln.de/manfred-thaller-dr-phil-prof [accessed 12 january 2013].
                        
141
   'informatica umanistica' (orlandi, 2003) and which they jointly define as 'the canon (or set of tools) needed to increase the knowledge agreed to be proper to a particular field' ([thaller], 1999, p. 25) leads them to a stronger identification with computer science than is currently acknowledged. the formalization of problems through algorithms and data representation by means of imposing structures on the data are identified  by thaller and orlandi as central methods for computing in the humanities ([thaller], 1999, p. 27; thaller, 2001 and 2004; orlandi, 2002).
this formalization of problems in particular has met with some criticism from text encoding and modelling theory. since the humanities are not problem-oriented, lou burnard argues that their methodologies cannot be formalized. instead, burnard puts hermeneutics and text encoding at the centre of humanities computing, two methods that are not shared with computing or any other science. hermeneutics is the study of interpretation that confers value on cultural objects (burnard, 2001, p. 32). burnard locates the starting point for the hermeneutic continuum in transcription and editing which are decisive and subjective acts of interpretation. the use of markup for the articulation and documentation of different semiotic systems in text62 offers the humanities a single formalism that reduces 'the complexity inherent in representing the interconnectedness of all aspects of our hermeneutic analysis, and thus facilitate[s] a polyvalent analysis' (burnard, 2001, p. 37). text encoding in this sense is different from the industrial preparation of a text for scholarship but constitutes a new form of scholarship, as sperberg-mcqueen has argued (sperberg-mcqueen, 1991, p. 34), and which mccarty has called 'a kind of epistemological modelling' (mccarty, 2003a). the text encoding initiative (tei) provides the humanities with dedicated markup models for the articulation and documentation that is, representation of different interpretations of and on text, and makes explicit a theory of text in a formalization that is processable by computers. so burnard does not argue for the formalization of the definition of problems, but for the formalization of texts and their interpretation into processable data structures.63
a second form of critique on what orlandi and thaller propose as central methods of humanities computing can be distilled from modelling theory which accepts a way of representing the full range of knowledge, even beyond what can be told explicitly and precisely (mccarty, 2004, p. 256) and thus beyond what can be formalized in algorithmic expressions. the problematizing purpose of modelling is furthered not only by failure
62 burnard discusses three interlocking semiotic systems of text: text as image, text as linguistic construct, and text as information structure (burnard, 2001, p. 33).
63 e.g. in the proposed formalization of texts in ordered hierarchies of content objects (ohco-thesis), and the interpretation of meanings of markup.
                        
        142
   but also by success, even when this is accidental and inexplicable from what is known at the time of constructing the model. the  fluid status of modelling as experimentation away from formalization plays an important role in the perspective on scholarship as a process rather than a product.64 the exponent of this is the model as tinkertoy65 which denotes its playful, experimental character. indeed 'the virtue of the noun 'model' is that in computationally based research, in which the work is fundamentally experimental, it defaults to its present participle 'modelling' and so to denoting that process' (mccarty, 2003b).
in their contribution to modelling theory for the humanities, meurig beynon, steve russ, and willard mccarty pleaded for 'reappraising computing from a perspective in which experience rather than logic plays a privileged role' (beynon et al., 2006, p. 145). in order to do so, they offered a perspective on computing they named human computing and de ned as 'a joint collaborative activity in which devices, typically electronic, augment what is the essentially human activity of the making of meaning' (beynon et al., 2006, p. 145). instead of the discovery of meaning (heuristics) that burnard proposed to realize by means of text encoding an approach which perceives scholarship primarily as a product66 they shifted the central concern of computing in the humanities to the making of meaning by means of what they called empirical modelling.67 by the perspective of human computing, computation transgresses its conventional functionality of executing algorithms that was introduced by the acceptance of the turing machine as its primary model and in which the human and the computer are in an alternating relationship to each other. it does so by including the human in the computational activity which they described as the 'continuous engagement and negotiation of the human with the computer through the experience of the construction and behaviour of the computer model' (beynon et al., 2006, p. 145). in this approach, the algorithmic formalization of problems as the central method of humanities computing is replaced by modelling of an empirical kind.
64 mccarty (1999a), for instance, sees text-encoding, that is, 'rendering phenomena computable by addition of metadata that unambiguously state what it is' as fundamental to the perspective of scholarship as product. he seems to overlook here that the actual textencoding is a transformative modelling activity which could produce a tinkertoy as well.
65 originally 'tinkertoy' was a construction toy with which children could build whatever their imagination could dream up and 'learn by exercising what we now think of as 'spatial intelligence'', http://www.toyhalloffame.org/toys/tinkertoy [accessed 12 january 2013]. as mccarty (2003b) noted, the term has been subsequently used 'to describe crude (or simply all physical) modelling techniques' (n. 1).
66 cf. mccarty (1999a).
67 for more information on empirical modelling see the empirical modelling website
at http://www2.warwick.ac.uk/fac/sci/dcs/research/em and the empirical modelling archive at http://www2.warwick.ac.uk/fac/sci/dcs/research/em/projects [accessed 12 january 2013].
                        
143
   this empirical modelling is supported by tools that engage with human cognitive processes and that allow for the 'experimental identification of relevant observables associated with some phenomenon and of reliable patterns of dependency and agency among these observables' (beynon et al., 2006, p. 146). thus, it resembles the research methods humanities scholars develop in approaching their objects of study in the personal and subjective relationship that is established between them.
thus far, it seems that modelling in general and data representation or text encoding in particular at least in their heuristically and epistemologically qualified meaning are crucial methods in humanities computing. as crude methods of practice they are not exclusive, that is identifying methods of humanities computing. therefore it is essential to distinguish text encoding as a scholarly (modelling) practice from the industrial data representation by markup which is in use, for instance, in the publishing industry. likewise, modelling which is computational by nature should be distinguished from the ancient art of model-building.
as noted earlier, the application of modelling and data representation may be specific  to humanities computing but they link back to the two general methods of humanities research, that is, analysis and synthesis respectively. with respect to the issue of synthesis, mccarty (2005) discusses scholarly commentary in digital editions as the most promising instantiation of synthesis in the humanities in which a certain degree of data representation, heuristics and modelling are combined into the scholarly reference work par excellence (mccarty, 2005, pp. 73-113).68 paradoxically, the characteristic which may identify the application of these common methods of humanities research as belonging to the field of humanities computing is indeed the computational aspect that we tried to transcend. if mccarty's book humanities computing is an attempt to provide a theory of humanities computing that incorporates this transgression, then it has failed to provide us with a clear, citeable, or formalized articulation of the methods of humanities computing which appeals to the problem of self-representation.69 curiously, mccarty presents exactly this problem of communication in two points in his preliminary agenda for humanities computing that provides the final perspective of the method of the field in his book. without a clear description of the formal method of humanities
68 the idealized structure of the commentary form mccarty discusses comprises the following major parts: '(a) a scholarly introduction to the work on which a commentary is offered; (b) an edited text of that work with textual apparatus; (c) the commentary itself, in the form of paragraph-length notes keyed to the text; and (d) the usual table of contents and index' (mccarty, 2005, p. 77).
69 mccarty's erudite and philosophical idiolect and stylistic fingerprint produces a dense and sometimes enigmatic prosaic style which takes much effort on the part of the reader to apprehend.
                        
        144
   computing, however, the field of activity will never succeed in providing popularized explanations of every activity and project undertaken in humanities computing, and in explaining and justifying what it does.
10. digital humanities
the same is true for digital humanities. the term has definitely but not definitively replaced humanities computing as a name for the field. there seems to be a common understanding of the term as referring to humanities research in the digital era, as opposed to traditional humanities research. however, the popularization and socialization of this new name for the field entails the risk of trivialization. the popular qualification 'digital' only relates to the technological (instrumental?) element of computation without using jargon language such as 'computer', 'computing' or 'computational'. this, however, does not solve the field's defining question, and even obscures the problem. although humanities computing was a more hermetic term than digital humanities, it had a clearer purview. humanities computing relates to the crossroads where informatics and information science met with the humanities and it had a history built on the early domains of lexical text analysis and machine translation, as we have seen above. digital humanities as a term does not refer to such a specialized activity, but provides a big tent for all digital scholarship in the humanities. the editors of a companion to digital humanities who introduced the term abruptly in 2004 as an expansion of what was commonly referred to as humanities computing, argue that the field 'rede ned itself to embrace the full range of multimedia' (schreibman et al., 2004, p. xxiii). it still remains the question, however, whether well established disciplines such as computational linguistics or multimedia and game studies will want to live under the big tent of digital humanities.
recently fred gibbs, the director of digital scholarship at the roy rozenzweig center for history and new media, introduced his classification of digital humanities definitions by warning that '[i]f there are two things that academia doesn't need, they are another book about darwin and another blog post about defining the digital humanities' (gibbs, 2011; chapter 21 in this volume). indeed, since blackwell's companion, a wide range of defining statements about the aims and nature of digital humanities (and sometimes why it differs from humanities computing) have been voiced.
the current volume harvests such defining contributions from journal articles and blog posts which are informed by roundtable discussions, conference panels, papers and posters, mission statements of digital
                        
145
   humanities centres and institutes, facebook walls, and tweets,70 and demonstrates that defining essays already constitute a genre of their own (kirschenbaum, 2010, p. 55; chapter 9 in this volume).71 among the recent and most elaborated additions to this genre is a four-part series of essays by patrik svensson in digital humanities quarterly. in these essays, svensson attempts to chart and understand the emerging field of digital humanities by examining the discursive shift from humanities computing to digital humanities in the first essay (svensson, 2009; chapter 7 in this volume), by exploring the broader landscape of digital humanities in the second essay (svensson, 2010), and by discussing the cyberinfrastructure for the humanities in general and for the digital humanities in particular in the third one (svensson, 2011). in the fourth essay, svensson presents 'a tentative visionary space for the future of the digital humanities' (svensson, 2012).
concluding that a broadly conceived digital humanities would necessarily include humanities computing with its focus on 'the instrumental, methodological, textual and digitalized' (svensson, 2009, p. 56) in his first essay, svensson also acknowledges that 'the epistemic commitments and conventions of a tradition', namely humanities computing, 'cannot easily be subsumed in another type of digital humanities' (svensson, 2010, p. 4). in other words, digital humanities is claiming a larger territory (svensson, 2009, p. 42).72 svensson thus argues that both terms are non-synonymous and that the discursive transition from humanities computing to digital humanities is not just a repackaging but a broadening of scope. he adds that the term is used by the digital humanities community as a collective name for activities and structures in between the humanities and information technology (svensson, 2009, p. 42).
the identification of humanities computing as a mainly instrumental application of computation to the text-based humanities is also present in tara mcpherson's typology of digital humanities. in her 2008 humlab lecture 'dynamic vernaculars: emergent digital forms in contemporary scholarship', mcpherson distinguishes among the computing humanities, the blogging humanities, and the multimodal humanities and sketches a certain interdependency among them. while the computing humanities refer to the long-lasting tradition of humanities computing with its focus on tools, standards, and interoperability (mcpherson, 2008, 0:10:00) and
70 assembled on the companion website to this volume, http://blogs.ucl.ac.uk/ definingdh.
71 volumes which gather such essays and discussions have also constituted a genre of their own. cf. berry (2012), gold (2012), lunenfeld et al. (2012).
72 the editors of a companion to digital humanities argue that the field 'has rede ned itself to embrace the full range of multimedia' (schreibman et al., 2004, p. xxii) with the launch of the name digital humanities.
                        
        146
   the blogging humanities refer to networked peer-to-peer writing, mainly by non-specialist computing humanists, the multimodal humanities combine these and investigate the computer as simultaneously a platform, a medium, and a display device. it is in this multimodal scholarship that mcpherson sees an agenda for digital humanities. we notice the same interdependency in svensson's analysis where he states that '[t]here are many humanities scholars involved in what may be called digital humanities who have no or little knowledge of humanities computing, and vice versa, many humanities computing representatives who do not engage much with current 'new media' studies of matters such as platform studies, transmedia perspectives or database aesthetics' (svensson, 2009, p. 7).
defining statements about digital humanities as those discussed so far commonly take references to humanities computing methodologies and scope as their starting point but hardly come to a definition. according to rafael alvarado that is because there is no definition of digital humanities. 'instead of a definition,' alvarado argues, 'we have a genealogy, a network of family resemblances among provisional schools of thought, methodological interests, and preferred tools, a history of people who have chosen to call themselves digital humanists and who in the process of trying to define the term are creating that definition' (alvarado, 2011).
therefore, alvarado calls digital humanities a social category, not an ontological one. he is supported by matt kirschenbaum, who de ned digital humanities in the 2011 day of digital humanities survey as 'a term of tactical convenience' (taporwiki, 2011). kirschenbaum in his essay 'what is digital humanities and what's it doing in english departments?' (kirschenbaum, 2010; chapter 9 in this volume) reminds us that the affirmation of digital humanities as the common name for the field was facilitated by the publication of a companion to digital humanities in 2004 (schreibman et al., 2004), the establishment of the alliance of digital humanities organizations (adho)73 in 2005, the launch of the digital humanities initiative by the neh in 2006, and the publication of digital humanities quarterly from 2007 onwards. only recently, the association for literary and linguistic computing joined this movement by changing its name to european association for digital humanities (eadh) in 2013. in a recent essay, kirschenbaum (2012) insists on the reality of circumstances in which the term is currently used to get things done:
at a moment when the academy in general and the humanities in particular are the object of massive and wrenching changes, digital humanities emerges as a rare vector for jujitsu, simultaneously serving to position the humanities at the
73 http://www.digitalhumanities.org [accessed 12 january 2013].
                        
147
   very forefront of certain value-laden agendas'entrepreneurship, openness and public engagement, future-oriented thinking, collaboration, interdisciplinarity, big data, industry tie-ins, and distance or distributed education'while at the same time allowing for various forms of intrainstitutional mobility as new courses are mooted, new colleagues are hired, new resources are allotted, and old resources are reallocated. (kirschenbaum, 2012)
not only the current use of the term, but also its origin was a moment of tactical convenience, as we learn from kirschenbaum's 'what is digital humanities' essay. apparently, blackwell's editorial and marketing people disliked the title a companion to humanities computing and wanted to name the volume a companion to digitized humanities. even the humanistic informatics was mentioned to cover the field, but as a compromise and to shift the emphasis away from simple digitization and complicated computing, john unsworth suggested a companion to digital humanities (kirschenbaum, 2010, pp. 56-7).
11. conclusion
as stated before, the problem of self-presentation and self-representation remains with digital humanities. willard mccarty, in his concluding chapter of humanities computing, defines a preliminary agenda for the field which shows kinship with mcpherson's multimodal humanities. most of the items in mccarty's agenda can be related or partly related to mcpherson's three types and even the big tent idea is implicitly advocated in mccarty's argument for a rapprochement between scholars and practitioners. in fact, mccarty's book demonstrates nicely that svensson's qualification of humanities computing as focused on 'the instrumental, methodological, textual and digitalized' is a reductionist perception. if one book has argued against an overemphasis of the instrumental use of the computer in the humanities and has promoted computing as a meaning-generating activity building on and bringing forth models of the world, it is mccarty's humanities computing.
the question 'what it is that we are doing in digital humanities and how does it relate to the world', is a question which should not be eschewed. even if it opens a can of worms, or, for the purpose of this essay, the gates of hell.
for the moment, we know that digital humanities tries to model the world around us through success and failure in order to arrive at a better understanding of what we know and don't know about humankind, their activities, artefacts, and record. and this can maybe serve as a definition of the field.humanities computing as digital humanities

abstract

this article presents an examination of how digital humanities is currently conceived and described, and examines the discursive shift from humanities computing to digital humanities. it is argued that this renaming of humanities computing as digital humanities carries with it a set of epistemic commitments that are not necessarily compatible with a broad and inclusive notion of the digital humanities. in particular, the author suggests that tensions arise from the instrumental, textual and methodological focus of humanities computing as well as its relative lack of engagement with the "digital" as a study object. this article is the first in a series of four articles attempting to describe and analyze the field of digital humanities and digital humanities as a transformative practice.

introduction

1
the humanities are undergoing a set of changes which relate to research practices, funding structures, the role of creative expression, infrastructural basis, reward systems, interdisciplinary sentiment and the emergence of a deeply networked humanities both in relation to knowledge production processes and products. an important aspect of this ongoing transformation of the humanities is humanities scholars increasing use and exploration of information technology as both a scholastic tool and a cultural object in need of analysis. currently, there is a cumulative set of experiences, practices and models flourishing in what may be called digital humanities. the research presented here explores the scope and direction of this emerging field as well as the role of humanities computing in this enterprise.
2
in this article, the first in a four-part series, i explore the discursive shift from humanities computing to what is now being termed the digital humanities, examining how this naming is related to shifts in institutional, disciplinary, and social organization. materials such as the humanist email list, journals, conference materials, principal texts, professional blogs and institutional websites provide an important empirical basis for the analysis. academic fields are partly produced, represented, reinforced, changed and negotiated through these modes of discourse. as will be evident from the analysis, the renaming of humanities computing to digital humanities brings with it a set of epistemic commitments that are not necessarily congruent with a broad and inclusive notion of the digital humanities. i suggest that interesting tensions arise from the instrumental, textual and methodological focus of humanities computing as well as its relative lack of engagement with the "digital" as a study object.
3
in the second article, i explore the broader landscape of the digital humanities through a discussion of digital humanities and digital humanists, associated traditions, personal encounters and importantly, through a suggested set of paradigmatic modes of engagement between the humanities and information technology: information technology as a tool, an object of study, an exploratory laboratory, an expressive medium and an activist venue.
4
the third article discusses cyberinfrastructure for the humanities more broadly  and for the digital humanities in particular  in relation to the current discourse of cyberinfrastructure, models of implementation and possible directions. the article also presents a fairly extensive case study of humlab  a digital humanities center at umea university. finally, tentative advice as to implementing and strategizing humanities cyberinfrastructure is offered.
5
in the fourth article, i explore the multiple ways in which the digital humanities have been envisioned and how the digital humanities can often become a laboratory and vehicle for thinking about the state and future of the humanities at large. some foundational issues, including the role of the humanities and changing knowledge production systems, are discussed and related to the development of the digital humanities. furthermore, a tentative vision of the digital humanities is presented. this vision is grounded in the article series as a whole as well as in the important collaborative possibilities and challenges that lie ahead of us.
6
together these four articles constitute an attempt to outline and critically discuss how the humanities interrelates with information technology in multiple ways, to understand the historical, conceptual, and disciplinary aspects of this interrelation, and to present an expansive model for the digital humanities.
background

7
one of things that has fascinated me for a long time is the range of origins, approaches and traditions associated with different varieties of digital humanities, ranging from textual analysis of medieval texts and establishment of metadata schemes to the production of alternative computer games and artistic readings of nanotechnology. an important rationale for this article series is to facilitate a discussion across various initiatives and disciplines and to make connections. there are many humanities scholars involved in what may be called digital humanities who have no or little knowledge of humanities computing, and vice versa, many humanities computing representatives who do not engage much with current "new media" studies of matters such as platform studies, transmedia perspectives or database aesthetics. few people will engage in activities across the board, of course, but it is important to have a sense of the growing disciplinary landscape, associated methodological and theoretical positions, and emerging collaborative possibilities. to me, this is an integral part of digital humanities as a project.
8
there are several good reasons for giving humanities computing the particular attention it receives in this article: its rich heritage, historical and current accomplishments, the sheer number of people involved, and the apparent discursive transition to "digital humanities." furthermore, any attempt at mapping an emerging field presupposes a discussion of disciplinary territory and ambitions, and humanities computing provides a particularly good starting point as it is relatively established and well-defined. and as we will see, many of the issues, considerations and parameters relevant to humanities computing are also relevant to digital humanities more generally.
9
in the following, we will start out from a particular example of humanities computing as digital humanities and associated epistemic commitments. some of these commitments are traced in the subsequent historical, institutional and contextual description of humanities computing. we will then move on to look at the renaming of humanities computing to digital humanities, which in turn will lead to a critical discussion of humanities computing with a particular focus on some points of tension between traditional humanities computing and an expansive notion of digital humanities. in conclusion, humanities computing will be briefly juxtaposed with a very different kind of digital humanities tradition.
setting the stage

10
the call for proposals for digital humanities 2009, the principal humanities computing conference, provides an illustrative example of how the disciplinary territory of digital humanities is being defined in relation to the tradition of humanities computing and how epistemic commitments can be manifested discursively.
11
epistemic cultures, as defined by [knorr cetina 1999, 1], are "those amalgams of arrangements and mechanisms  bonded through affinity, necessity, and historical coincidence  which, in a given field, make up how we know what we know " (original emphasis). we are thus concerned with ways in which knowledge is created, represented and defended. epistemic cultures are constructed and maintained through, among other things, the epistemic commitments of participating scientists as part of the means by which alignments are made between academic disciplines, the fields of enquiry that they represent, and shared notions about what constitutes valid research [ratto 2006]. in the following, the epistemic commitments of humanities computing and digital humanities are mainly traced through looking at different modes of discourse. while these modes may have different functions and intended audiences, they collectively add to the analysis.
12
the digital humanities 2009 call is divided into three parts. the first part provides a broad and relatively open definition of the digital humanities.
the international programme committee invites submissions of abstracts of between 750 and 1500 words on any aspect of digital humanities, broadly defined to encompass the common ground between information technology and problems in humanities research and teaching.
as always, we welcome submissions in any area of the humanities, particularly interdisciplinary work. we especially encourage submissions on the current state of the art in digital humanities, and on recent new developments and expected future developments in the field.
13
the invitation relates to "any aspect of digital humanities" which is loosely defined as the common ground between information technology and problems in humanities research and teaching. interdisciplinary contributions are particularly encouraged. as expected, the second part provides a higher level of specificity.
suitable subjects for proposals include, for example,
text analysis, corpora, corpus linguistics, language processing, language learning
libraries, archives and the creation, delivery, management and preservation of humanities digital resources
computer-based research and computing applications in all areas of literary, linguistic, cultural, and historical studies, including electronic literature and interdisciplinary aspects of modern scholarship
use of computation in such areas as the arts, architecture, music, film, theatre, new media, and other areas reflecting our cultural heritage
research issues such as: information design and modelling; the cultural impact of the new media; software studies; human-computer interaction
the role of digital humanities in academic curricula
digital humanities and diversity
14
here we are presented with a narrowing down of what was described in the first part. this is common in conference calls as a way of indicating the particular focus of the conference, of course, although it is difficult to discern any clear thematic delimitation in this particular case. we are thus concerned with a fairly broad range of possible topics. however, the ordering and phrasing of these topics suggest a specific tradition or framework, and an associated set of epistemic commitments. for instance, it is not by accident that text analysis comes first and that phrases such as "computer-based research" and "use of computation" are used. even so it could be argued that much of what be included in a broad notion of digital humanities could be subsumed under these topics, and that particularly the sixth topic  research issues  opens up the scope to areas such as new media studies. but the placement, exact wording (e.g. "the cultural impact of new media") and the broader context may not make these potential conference participants feel targeted unless they already have a relation to the community and humanities computing.
15
in the third part of the call for proposals follows a much more precise definition of digital humanities and associated topics:
the range of topics covered by digital humanities can also be consulted in the journal of the associations: literary and linguistic computing (llc), oxford university press.

the journal literary and linguistic computing has been a key publication for humanities computing for a long time. however, defining digital humanities through the topics presented in llc clearly excludes many other initiatives and developments in the intersection of the humanities and information technology and suggests a very particular tradition, institutional grounding and epistemic culture.[1] moreover, this level of narrowing down is clearly not congruent with the description of digital humanities given in the first part of the call, which may be said to be less obviously situated in the tradition of humanities computing and associated epistemic commitments.
history and paradigm

16
the partial institutionalization of humanities computing has resulted in academic departments or units, annual conferences, journals, educational programs and a rather strong sense of communal identity. these are all qualities that are typically associated with the establishment of a new discipline (cf. [klein 1996, 57]). the following excerpt from a description of a 1999 panel organized by the association for computing the humanities seems to confirm this analysis:
empirically, humanities computing is easily recognized as a particular academic domain and community. we have our professional organizations, regular conferences, journals, and a number of centers, departments, and other organizational units. a sense for the substance of the field is also fairly easy to come by: one can examine the proceedings of ach/allc conferences, issues of chum and jallc, the discussions on humanist, the contents of many books and anthologies which represent themselves as presenting work in humanities computing, and the academic curricula and research programs at humanities computing centers and departments. from such an exercise one easily gets a rough and ready sense of what we are about, and considerable reassurance, if any is needed, that indeed, there is something which we are about.[2]

17
communal identity, of course, is built over time, and history and foundational narratives play an important role in this process. father roberto busa is typically cited as the pioneer of the field of humanities computing, and his work dates back to the late 1940s:
during the world war ii, between 1941 and 1946, i began to look for machines for the automation of the linguistic analysis of written texts. i found them, in 1949, at ibm in new york city.  [busa 2004, xvi]

18
in this foundational story, two important epistemic commitments of humanities computing are established: information technology as a tool and written texts as a primary object of study (for linguistic analysis). commitments such "computer as instrumental tool" and "text as object" end up helping decide what are legitimate types of questions and study objects for the field, and how work and relevant institutions are organized.
19
the journal computers and the humanities was started as early as in 1966 and, interestingly, it seems as if early issues were not as textually oriented as one might have assumed. early articles include "pl/i: a programming language for humanities research," "art, art history, and the computer" and "musicology and the computer in new orleans" (all from 1966-1967). thirty years later we find articles such as "the design of the tei encoding scheme," "current uses of hypertext in teaching literature," "neural network applications in stylometry" and "word frequency distributions and lexical semantics" (all from 1995-1996). in 2005, this journal was renamed language resources and evaluation, and had by this time lost its status as one of the "official" journals for humanities computing. in one of the obituaries, willard mccarty applauds the first 25 years of the journal and comments on the editors final statement (which points the difficulty of maintaining the broad scope of the journal):
chum's astonishing denial of a future for humanities computing comes in the same year as the blackwell's companion to digital humanities. [] if anything, the development of chum since then suggests rather the opposite  a narrowing down from the breadth of humanistic interests, across the full range of disciplines, to a sharp focus on material often closer to computational linguistics than anything else  and often too technical for all but the specialist to read. this narrowing does not reflect the field.  [humanist 18.615]

20
in other words, computers and the humanities was seen as having taken a direction not fully compatible with the epistemic tradition of humanities computing. indicatively, in a call for papers from 1998,[3] there is a special invitation for state-of-the-art surveys, and the only example given is "current approaches to punctuation in computational linguistics." also, this happened at about the same time as the alliance of digital humanities associations (adho) was formed, and another important reason for the "demise" of computers and the humanities was that it was strategically, financially and institutionally advantageous to make literary and linguistic computing and not computers and the humanities the principal humanities computing journal.[4] indeed, these reasons were probably more important than the perceived incompatibility between humanities computing at large and computers and the humanities. nevertheless, the result was that for a few years, humanities computing only had one principal journal.
21
the journal literary and linguistic computing has from its inception focused on textual and text-based literary analysis  as you would expect from its title. it was established in 1986 by the association for literary and linguistic computing (itself established in 1973). this journal has clearly played an important role in establishing the field of humanities computing  not only in offering a publication venue, institutional structure and academic exchange but also in publishing self-reflective articles on the role, organization and future of humanities computing. as we saw earlier, the journal has even been used to define the digital humanities  thus in a sense transferring the epistemic culture of the journal and associated field to the "new" field.
22
as important as these printed journals have been for establishing humanities computing as a field, humanities computing representatives were also early adopters of communication technologies such as email lists. the first message on the humanist list was sent on may 13, 1987 by founding editor willard mccarty, making it one of the first academic email lists to be established. currently about 1600 people subscribe to the humanist list[5] which is an email list with consistently high quality, carefully organized threads and an often lively discussion.[6] although the range of topics is very broad it is fair to say that there is persistent and fundamental interest in textual analysis and related matters. as mccarty himself points out, humanist facilitates an ongoing, low-key and important discussion:
we're always worrying ourselves about whether humanities computing has made its mark in the world and on the world. it seems to me, however, that quiet change, though harder to detect, is sometimes much better and more powerful in its effects than the noisy, obviously mark-making, position-taking kind. if during these 17 years humanist has contributed to the world, it has done so very quietly by nature, like conversation, leaving hardly a trace.  [humanist 18.001]

23
here it is also rather obvious that "humanities computing" serves as an identifying label and collaborative sentiment for the humanist community. we will soon return to this label (and an ongoing relabeling process) as well as the worry or concern that mccarty mentions but first a brief look at another major institution in this field.
24
one of the most important venues for humanities computing have been the annual conferences jointly organized by the association for literary and linguistic computing (allc) and the association for computers and the humanities (ach). originally these organizations ran their own conference series, but from 1996 they started a joint conference series. from 2008, the society for digital humanities/socit pour l'tude des mdias interactifs (sdh/semi) became a third organizing association. these three associations are all members of the alliance of digital humanities associations. it is quite clear that these conferences predominantly address textual analysis, markup, retrieval systems and related areas. a simple frequency analysis based on titles of papers and sessions from 1996 to 2004 shows us that frequent non-functional words include text (56), electronic (53), language (30), markup (28), encoding (27), tei (23), corpus (22), authorship (18), xml (18), database (13) and multimedia (11). in comparison there is one instance of game and two instances of the plural form games. this is a rather crude measurement, of course, but it does give us a sense of the overall orientation. a more careful look at the 2005 conference (at university of victoria, bc) does not seem to contradict this sketch. for instance, the themed sessions that extended more than one program slot were "authorship attribution," "libraries, archives & metadata," "computational linguistics and natural language processing," "encoding & multiculturalism," "scholarly projects" and "visualisation & modeling." one-slot themed sessions included "automation," "text & technology," "textual editing & analysis," "interface design" and "hypertext".[7] yet another example is the 2008 digital humanities summer institute [humanist 21.469]. here the focus is on text encoding, transcription, and corpus text analysis in five out of the eight offerings in the curriculum. the other three sessions take up digitization fundamentals, multimedia and large project planning.
25
while, journals, conferences and academic associations play an important role in creating and maintaining an academic field and community, another important factor is the ways in which a field has been institutionalized. in the case of humanities computing, this has been a long and partly uncertain process, which has clearly shaped the field.
institutional models

26
in organizational terms, humanities computing enterprises have been institutionalized in many different ways. and, of course, institutions develop over time. a useful resource is willard mccartys and matthew kirschenbaums "institutional models for humanities computing" [mccarty & kirschenbaum 2003]. here a number of questions or criteria are used to list and categorize humanities computing institutions. the first category incorporates academic units that do research, teaching and collegial service. also "[s]ome members of these units hold academic appointments either in or primarily associated with humanities computing." examples include the center for computing in the humanities, kings college london, and the institute for advanced technology in the humanities. even though it is said in the document that "[n]o judgement is expressed or implied as to the worth of the centres under consideration," it could probably be argued that this first category serves as a role model (based on the way criteria are created and presented, the ordering of the categories and a broader humanities computing context).
27
historically, and to some extent contemporarily, it would seem that a prototypical organizational form is a humanities computing unit or center affiliated with a school of liberal arts or humanities. often such units provide service to the rest of the school and this rather instrumental function has typically been primary. of course, there might have been development in many other directions over time, but this basic function cannot easily be dismissed. a prominent example would be the humanities computing unit at oxford university whose roots go back to the 1960s and which was closed (or transformed) in 2002. [burnard 2002] describes the final stages of this development:
at the start of the new millenium, the hcu employed over 20 people, half of them on external grants and contracts valued at over 350,000 annually. with the advent of divisionalization, however, it faced a new challenge and a new environment, in which oucs, as a centrally-funded service, must take particular care to meet the needs of the whole university, in a way which complements the support activities funded by individual divisions, rather than competing with or supplanting them. our strategy has been to focus on areas where the hcu's long experience in promoting better usage of it within one discipline can be generalized. in 2001, we set up a new learning technologies group, to act as a cross-disciplinary advocacy and development focus for the integration of it into traditional teaching and learning. this new ltg is now one of four key divisions within the new oucs, additionally responsible for the full range of oucs training activities.

28
the status of such academic units, of course, is not normally on the same level as (traditional) departments which tend to be the privileged academic organizational unit. in many cases humanities computing units have been seen as service units with a rather instrumental role and representatives find themselves having to present their field in such a way as to maintain financial support as well as their share of integrity and independence. frequently, like in the case above, academic units which are seen as having a technological service function are susceptible to different kinds of organizational changes and budget cuts. for instance, the central university administration might question whether the most efficient organizational structure is to have departments and faculties run their own computer support functions or whether it is more efficient to adopt a more centralized model. also humanities computing units that have several functions might have to cut back on the more research-oriented activities because, after all, technical support is more instrumental (and sellable/buyable) and there might not be enough explicit interest from humanities departments to motivate a more research and methodology focused function. there are many examples of changes like these (see [flanders & unsworth 2002] for some other examples and a further discussion). several prominent service-based units, including the humanities computing unit at oxford university and centre for computing in the humanities at university of toronto, have been closed down (or radically reformed) over time and this vulnerable position is part of the shaping of humanities computing.
29
while it is fair to say that the present institutional landscape is rather diverse and expansive, it is also important to acknowledge that the ratio of thriving humanities computing environments and initiatives at universities in europe and the united states is still very low in relation to the whole of the humanities; something that may or may not be seen as a problem. taking sweden as an example, there seems to be only one traditional humanities computing unit in the country (at gothenburg university) at present. most of the growth seems to happen in places where there is no or little humanities computing legacy (blekinge institute of technology and sdertrn university college). my own environment, humlab at ume university, does relate to humanities computing, but also to many other influences, and most of the ph.d. students, for instance, would probably not see themselves as primarily involved in humanities computing. most of them do subscribe to the humanist, however.
the question of autonomy

30
a related and much-discussed issue  highly relevant to digital humanities generally and to humanities computing as digital humanities  concerns whether humanities computing should be independent and possibly an academic discipline in its own right or whether it should primarily interrelate with existing humanities departments. this discussion has partly been fueled by the need for academic status to create academic positions and a sense of not wanting or needing to be reliant on traditional and slow-moving departments and disciplines.[8] in fact, these disciplines may not even be considered suitable for dealing with relevant study objects and research issues, or appropriate methodologies:
to study the effects and consequences of digital technology on our culture, and how we are shaping these technologies according to our cultural needs, we can now begin to see the contours of a separate, autonomous field, where the historical, aesthetic, cultural and discursive aspects of the digitalisation of our society may be examined. that way, the field of humanistic informatics may contribute to the goal of the humanities, which is the advancement of the understanding of human patterns of expression. we cannot leave this new development to existing fields, because they will always privilege their traditional methods, which are based on their own empirical objects.[9]  [aarseth 1997]

31
another argument for not involving all of the humanities may be that it is not seen as an efficient model. [mcgann 2001, 7] tells us about strategies adopted when the institute for advanced technology in the humanities (iath) at university of virginia was started. alan batson, department of computer science at uva, argued that trying to involve everyone (distribute resources evenly) would be to replicate 30 years of failure; providing it resources to people who are not interested in them or do not want to explore them does not work.
iath was founded as a resource for people who had already made a commitment to humanities computing, a commitment defined practically by an actual project with demonstrable scholarly importance.  [mcgann 2001, 9]

32
the tension between trying to involve as many as possible and making a difference through engaging people who have already shown an interest is basic and recurrent. naturally, any enterprise of this kind is dependent on the local environment. there is obviously a significant difference between being an autonomous academic unit and a service-based or organization. in practice most humanities computing units are probably somewhere in between. also, the "service" function can, of course, be very complex and should not be trivialized. mccarty talks about "practice" and "practitioners," and such terminology might be more suitable for many of the service-like functions more directly related to the humanities computing enterprise. he stresses the importance of methodological knowledge and says that "[t]he practitioner learns a specific but generalizable method for tackling problems of a certain kind"  [mccarty 2005, 120]. this focus on methodology and associated tools is common in humanities computing, and arguably part of the epistemic commitments of the field that fundamentally shape the way humanities computing relate to the rest of the humanities and to other work in the humanities and information technology.
approaching the digital humanities

33
as we noted earlier "humanities computing" has been a strong common denotation for much of the work and community described above. in his humanities computing, willard mccarty describes the development from "computers and the humanities" via "computing in the humanities" to "humanities computing." he characterizes these three denotations as follows: "when the relationship was desired but largely unrealized" (computers and the humanities), "once entry has been gained" (computing in the humanities) and "confident but enigmatic" (humanities computing) [mccarty 2005, 3]. i have argued elsewhere [humanist 17.111] that juxtaposition (as in the first stage) does not necessarily have to indicate separated entities and that "humanities computing" has an instrumental ring to it. also, "humanities computing" does not necessarily seem to include many of the approaches and materials that interest many humanities scholars interested in information technology (and computing). of course, these arguments are related to the ambitions and scope of the field you are trying to denote.
34
from this point of view, it is interesting to note that humanities computing representatives currently seem to be appropriating the term digital humanities. prominent examples of use of the new identifier include the relabeled allc/ach conference (from 2006 onwards entitled "digital humanities"), a new book series called "topics in digital humanities," a new comprehensive website http://www.digitalhumanities.org sponsored by the major humanities computing associations, the peer-reviewed journal digital humanities quarterly, the massive, edited volume a companion to digital humanities [schreibman, siemens & unsworth 2004], and the recent renaming of the candian consortium for computers in the humanities into the society for digital humanities. the denotation has certainly been used before (at university of virginia among other places), but it seems to be employed more broadly now and in a more official and premeditated fashion. an important indication of the spread of the term and institutionalization of the field can be seen in the establishment of the office of digital humanities by the national endowment for the humanities (us) in 2008. a broader analysis of different varieties of digital humanities will be returned to in the second article in this series.
35
looking at issues 1-20 of the humanist [10] and instances of humanities computing versus digital humanities, the following figures emerge: 304/2 (1997-1998), 343/3 (2000-2001), 566/16 (2001-2002), 283/15 (2002-2003), 280/19 (2003-2004), 363/45 (2004-2005), 130/44 (2005-2006) and 110/90 (2006-2007). the first instances of digital humanities in issues 11 and 14 (1997-1998 and 2000-2001 respectively) refer to nominal constructions such as digital humanities object and digital humanities environment. while we should be careful about how to interpret crude quantitative data like these, it is fairly clear that humanities computing for a long time was the predominant term and still is frequent, but that we are moving towards an increased use of digital humanities (relative to humanities computing). the retained and frequent use of the older term points to a discrepancy between the over-the-board institutional renaming of the field described above and the communitys use of the term as evidenced in the humanist material.
36
this discrepancy or co-existence[11] is also evident if you look at the blackwell's a companion to digital humanities from 2004. there are about twice as many instances of humanities computing as digital humanities (139/68). the internal distribution of the terms is more interesting and can easily be explored using the online version of the companion. for instance, humanities computing is predominantly used in the section where the contributors are described, while digital humanities is much more common than humanities computing in the introduction (called "the humanities computing and the digital humanities: an introduction"). these two texts represent very different genres. the notes on contributors section is largely a venue for self representation and presentation. the introduction is where the (new) field of digital humanities is being described and advocated (by the editors of the volume). in the history section (12 chapters in total) it is clearly the history of humanities computing that is told (58 instances of humanities computing versus 1 instance of digital humanities). the section on principles (7 chapters) is primarily humanities computing-focused (23/4) as the main topics are text analysis, encoding, classification and modelling. the final two sections  on applications and production, dissemination, archiving  contain fewer instances of either term. one possible reason may be because these sections are more grounded in actual practice. also, it is clear that individual preference plays an important role. again, we are concerned with simple, quantitative measurements, but there is definitely a picture emerging.
37
a pertinent question is whether the discursive transition from humanities computing to digital humanities is mainly a matter of repackaging (humanities computing), or whether the new label also indicates an expanded scope, a new focus or a different relation to traditional humanities computing work. the editors of the book series "topics in the digital humanities" indicate an ongoing change:
humanities computing is undergoing a redefinition of basic principles by a continuous influx of new, vibrant, and diverse communities of practitioners within and well beyond the halls of academe. these practitioners recognize the value computers add to their work, that the computer itself remains an instrument subject to continual innovation, and that competition within many disciplines requires scholars to become and remain current with what computers can do.  [humanist 19.052]

38
the book series announcement as a whole, however, maintains a focus on the computer as a tool and humanities computing methodologies. the epistemic commitment to technology as tool is also clearly evident from "[t]hese practioners recognize the value computers add to their work."
39
unsurprisingly, it is difficult, possibly irrelevant, to pinpoint the meaning of a term in change, but it is nevertheless relevant to look at how such terms are introduced and used by an academic community. it is obvious that the term digital humanities, as used by the humanities computing community, often serves as an overarching denotation in book and journal titles, etc., while humanities computing is often used in the actual narrative.
40
the territory of the term is being defined and negotiated by institutional entities such as the journal digital humanities quarterly. the following text, which also suggests ongoing change, comes from the very first editorial of dhq in the inaugural issue:
digital humanities is by its nature a hybrid domain, crossing disciplinary boundaries and also traditional barriers between theory and practice, technological implementation and scholarly reflection. but over time this field has developed its own orthodoxies, its internal lines of affiliation and collaboration that have become intellectual paths of least resistance. in a world  perhaps scarcely imagined two decades ago  where digital issues and questions are connected with nearly every area of endeavor, we cannot take for granted a position of centrality. on the contrary, we have to work hard even to remain aware of, let alone to master, the numerous relevant domains that might affect our work and ideas. and at the same time, we need to work hard to explain our work and ideas and to make them visible to those outside our community who may find them useful.  [flanders et al 2007]

41
this is an inclusive and open definition which also suggests a particular community, associated history, changing boundaries and possibly some fence keeping (imposing a notion of centrality or non-centrality and through identifying "we" and "them"). although no direct reference is made in the text, it is rather clear that the tradition implicitly referred to is humanities computing. the interest in dialogue indicated in the editorial is clearly important to the development of the whole field. importantly, for a broad notion of digital humanities and a consorted effort, this dialogue must not only incorporate humanities computing as digital humanities and other varieties of digital humanities, but must also take place across a disciplinary landscape that additionally includes quite a number of initiatives and people that might not primarily classify what they do as digital humanities. indeed, not even everyone associated with the enterprises being subsumed under the label digital humanities might be comfortable with that categorization.
42
in any case, the new name definitely suggests a broader scope and it is also used in wider circles as a collective name for activities and structures in between the humanities and information technology.[12] and as we have seen in this analysis, there are many examples of humanities computing as digital humanities claiming a larger territory.
humanities computing as digital humanities

43
if humanities computing is to be taken as a more general digital humanities project it seems relevant to carefully consider the scope, implementation and ambition of the paradigm. also, regardless of this perspective, there are certain characteristics of the paradigm that deserve critical attention and discussion. the four issues presented below touch on some of the disciplinary boundaries and epistemic culture of humanities computing and may possibly challenge some established perceptions of humanities computing. in any case, what follows is not so much a criticism of a paradigm as an exploration of boundaries and possibilities. it should also be added that the points discussed here have a bearing on digital humanities more generally.
44
first, humanities computing as a whole maintains a very instrumental approach to technology in the humanities. in her introductory chapter in the volume digital humanities, susan hockey says that this is not the place to define humanities computing, and continues, "[s]uffice it to say that we are concerned with the applications of computing to research and teaching within the subjects that are loosely defined as 'the humanities,' or in british english, 'the arts' "  [hockey 2004, 3] (italics added). hockeys description is indicative of a paradigm in which information technology is typically not seen as an object of study, an exploratory laboratory, an expressive medium or an activist venue. rather, technology has this basic and epistemically grounded role as a tool and much of humanities computing is about using these tools, helping others to use them and, to some extent, developing new tools (and methodologies). many of these tools, such as concordance programs, have a rather long and distinguished history, and there has not necessarily been a great deal of radical change over time (see [mccarty 1996]). it could be argued that the focus of traditional humanities computing is not innovating new tools, but rather using and developing existing ones. also a fair proportion of the development seems to occur on a structural or meta-data level. examples include text encoding and markup systems. of course work on this level has fundamental implications for the development and use of tools.
45
text encoding is typically seen as a core element of humanities computing. koenraad de smedt says that "text encoding seems to create the foundation for almost any use of computers in the humanities"  [de smedt 2002, 95].[13] classifications such as the major text encoding initiative (tei) involve very basic theoretical and methodological challenges [mcgann 2006] and there have also been calls for the development of more innovative tools based on these and other schemas [rockwell 2003]. rockwell stresses the importance of moving beyond existing personal tools, making community and server based tools more available, allowing for playful exploration and encouraging critical discussion of tools. clearly there is a need for such a development, and while there are some exemplary projects there is a need for further development, discussion of best practice and further critical analysis. for instance, it would be interesting to see more integration with web 2.0 thinking and platforms.[14] work in interaction and participatory design as well as methodologies such as rapid prototyping. an interesting, current example of methodological innovation is rockwells and sinclairs work on extreme text analysis.[15]
46
it might also be argued that traditional humanities computing has not primarily been concerned with interface and how things look and feel  the materiality of the tools. kirschenbaum says that "the digital humanities have also not yet begun [] to initiate a serious conversation about its relationship to visual design, aesthetics, and, yes, even beauty"  [kirschenbaum 2004, 532]. mcgann asserts that "[d]igital instruments are only as good as the interfaces by which we think through them"  [mcgann 2006, 1567]. there have also been calls for tools with more far-reaching and radical scope than the ones that humanities computing typically provides. drucker and nowviskie point out that "[w]e are not only able to use digital instruments to extend humanities research, but to reflect on the methods and premises that shape our approach to knowledge and our understanding of how interpretation is framed"  [drucker & nowviskie 2004, 432].
47
second, it has often been pointed out that what brings humanities computing together is largely a common interest in methods, methodology, tools and technology. this partly follows from an instrumental orientation, of course, and there is no reason to question the methodological commons as a valuable interdisciplinary focus and productive collaborative sentiment. however, this strong methodological focus fundamentally affects the way humanities computing operates and relates to other disciplines. the most serious implication is that a predominantly methodological link to other disciplines may not integrate many of the specific issues that are at the core of these disciplines. it could be argued that this makes it more difficult for humanities computing to reach out more broadly to traditional humanities departments and scholars. while there will always be interest in methods and technology, the actual target group  humanities scholars with an active interest in humanities computing tools and perspectives  must be said to be relatively limited.[16] in an interesting and provocative paper, [juola 2008, 83] argues that the emerging discipline of "digital humanities" has been emerging for decades and that there is a perceived neglect on the part of the broader humanities community. while he is appreciative of the work done in humanities computing, he also finds that
for the past forty years, humanities computing have more or less languished in the background of traditional scholarship. scholars lack incentive to participate (or even to learn about) the results of humanities computing.

48
looking at text analysis, rockwell points out that "text-analysis tools and the practices of literary computer analysis have not had the anticipated impact on the research community"  [rockwell 2003, 210]. juolas analysis shows that citation scores for humanities computing journals are very low and he also points out that the american ivy league universities are sparsely represented in humanities computing publications and at humanities computing conferences. it could be argued, however, that the lack of citations is partly due to the fact that humanities scholars who use humanities computing tools might not be inclined to cite the creators of these tools. this is especially true if no written work on associated methodology (or theories) has been employed in the research.
49
a relevant question, of course, is whether humanities computing wants and needs to reach out to the humanities disciplines.[17] this relates to the earlier discussion of autonomy and discipline or not. there seems, however, to be rather strong support for expanding the territory and for achieving a higher degree of penetration. furthermore, if the methodology and tools are central to the enterprise it seems counter-intuitive to disassociate yourself from many of the potential users (and co-creators) of the tools. it is evident from his discussion of possible high-profile "killer applications" that juola shares an interest in the development of a new or evolved kind of tools with drucker and nowviskie and others. it could be argued that it would be beneficial to have tools or applications that relate more directly to some of the central discipline-specific challenges of the various humanities disciplines. such a development would probably lead to somewhat less focus on methodology, a tighter integration of humanities computing and humanities disciplines[18] and possibly more tools and applications with a rich, combined theoretical, experiential and empirical foundation.
50
third, humanities computing has a very strong textual focus. given the history and primary concerns of the field as well as the textual orientation of much of the humanities this is not very surprising. traditional text is clearly a privileged level of description and analysis. in her analysis of humanities computing, which is partly corpus-based, terras writes that "humanities computing research is predominantly about text"  [terras 2006, 236]. while this is true, there has certainly been an increased interest in multimedia and non-textual representation. this interest may, for instance, be manifested in the form of metadata schemes for visual material or, increasingly, the interest in using geographical information systems in humanities computing. reference is sometimes made to different technologies and methods (3d-modeling, gis, animation, virtual reality etc.) but these are not necessarily integrated in practice. for instance, jessop says that "the research potential of working with digital tools for handling spatial data has been explored in only very limited contexts"  [jessop 2007, 4]. there are many exceptions and prolific scholars with a strong commitment to these issues but this cannot be said to be true of most of humanities computing. there is also a risk that other media are handled much in the same way as text (e.g. another object type to encode) or merely subservient to text following a very strong epistemic commitment to text as object. here follows a rather text-focused discussion of images in relation to the history (and future) of humanities computing:
there are of course many advantages in having access to images of source material over the web, but humanities computing practitioners, having grown used to the flexibility offered by searchable text, again tended to regard imaging projects as not really their thing, unless, like the beowulf project  [kiernan 1991], the images could be manipulated and enhanced in some way. interesting research has been carried out on linking images to text, down to the level of the word  [zweig 1998]. when most of this can be done automatically we will be in a position to reconceptualize some aspects of manuscript studies. the potential of other forms of multimedia is now well recognized, but the use of this is only really feasible with high-speed access and the future may well lie in a gradual convergence with television.  [hockey 2004, 15]

51
there is nothing wrong with a textual focus, of course, but it does have effects on the scope and penetration of humanities computing. the so-called "visual turn" [19] or research on multimodal representation does not seem to have had a large impact on humanities computing. one reason is probably because there is little interaction between these communities and because it is difficult to conceptualize and develop tools for these kinds of framework. more generally, there seems to be an increasing interest in non-textual and mixed media in the humanities and elsewhere (see for instance research on remediation, transor crossmedia texts, digital art and the current interest in "mashups"). and, needless to say, most native digital media are not pure text while humanities computing through focusing on text in its digitalized and encoded form could be said to privilege a rather "pure" (if annotated and structured) form of text. it seems that there should be considerable opportunities in this area for humanities computing  both for innovative tools and thinking  but also in relation to making a strong case for the need for considerable cyberinfrastructure in the humanities.[20] furthermore, there is clearly a need for people with expert competence and interest in structuring, annotating and managing data. it is exciting to see that interest in non-textual representation and analysis seems to be growing in humanities computing. it seems worthwhile to support this development  at least if the vision is an expansive and inclusive humanities computing/digital humanities. such a development would not have to preclude a retained textual focus, of course.
52
my fourth and final point relates to data and material used in humanities computing  or, put another way, the objects of study of humanities computing and associated disciplines. mccarty distinguishes between four data types in his discussion of a methodological commons: text, image, number and sound [mccarty 2005, 136]. it is characteristic of the model that the source materials and approaches of the disciplines are reduced these four data types and a "finite (but not fixed) set of tools for manipulating them".[21] this touches on a tendency to subscribe to formal and science-driven models of knowledge production in humanities computing (where text is the principal object of study):
applications involving textual sources have taken center stage within the development of humanities computing as defined by its major publications and thus it is inevitable that this essay concentrates on this area. nor is it the place here to attempt to define interdisciplinarity, but by its very nature, humanities computing has had to embrace "the two cultures," to bring the rigor and systematic unambiguous procedural methodologies characteristic of the sciences to address problems within the humanities that had hitherto been most often treated in a serendipitous fashion.  [hockey 2004]

53
as we have already seen and as the above quote reinforces, text is a privileged data type in humanities computing. furthermore it could be argued that humanities computing is mainly interested in digitalized texts (or in some cases, digitalized historical sites etc.) and not material that is natively digital. born digital material would include computer games, blogs, virtual worlds, social spaces such as myspace, email collections, websites, surveillance footage, machinima films and digital art. most of these "objects" are studied and analysed within different kinds of new media settings and to me this is an interesting in-between zone. would humanities computing be interested in engaging more with new media scholars? there is certainly a need for well-crafted tools for studying online life and culture. why does there not seem to be any software for doing comparative analysis and interpretation of computer games, for instance?[22] how can machinima films be tagged and related to the cultural artefacts to which they reference? how do we systemize and contextualize email archives?[23] can social software platforms be adapted to humanities computing needs? can multimodal and multi-channel communication be tracked, tagged, interrelated and made searchable in any consistent way?
54
i find the intersection between humanities computing and new media studies intriguing. there is some new media-like work going on in humanities computing but it is relatively marginal and there are few tools available. a more complete and multifaceted engagement might stimulate more theoretical work in humanities computing. rockwell makes a case for the importance of such an engagement:
digital theory should not be left to new media scholars, nor should we expect to get it right so that we can go back to encoding or other humanities disciplines. theorizing, not a theory, is needed; we need to cultivate reflection, interruption, standing aside and thinking about the digital. we dont need to negotiate a canon or a grand theory, instead i wish for thinking about and through the digital in community.  [rockwell 2004]

55
regardless of whether such an engagement involved theory or mainly methods and tools, it seems that there might be mutual gains. not least would humanities computing be able to draw more on a growing interest in digital culture and the "technological texture" that don ihde postulates. a further possible result would be a more robust link to humanities disciplines through also working in a field where there are many current and important research challenges in relation to the digital (e.g. participatory culture, surveillance societies, gender and technology, and emerging art and text forms).[24]
56
the epistemic commitments of humanities computing are not limited to points discussed above, however these are particularly relevant for the discussion of humanities computing as digital humanities[28]. a broadly conceived digital humanities would necessarily include the instrumental, methodological, textual and digitalized, but also new study objects, multiple modes of engagement, theoretical issues from the humanities disciplines, the non-textual and the born digital.
multiple identities and risk taking

57
let us briefly contrast humanities computing with a rather different kind of institutional setting and epistemic tradition. anne balsamo writes about the georgia institute of technology in the article "engineering cultural studies: the postdisciplinary adventures of mindplayers, fools, and others." more specifically she relates the story, tensions and context of the program in science, technology, and culture offered in the school of literature, communication and culture (lcc) at georgia tech. partly this is done through the work of cyberpunk science fiction writer pat cadigan.
58
lcc used to be an english department and was transformed in 1990. balsamo discusses the different identities that faculty wear and the complex interrelations associated with being a humanities representative at a predominantly technical school. for instance, the institutional position requires lcc faculty to be committed to traditional humanities values, in order not to give engineering schools arguments for reducing or doing away with the humanities requirement. the lack of a stable identity is the result of different roles and an interdisciplinary setting, and it resonates with the lack of stable identity that seems to be such an integral part of humanities computing. the interdisciplinary meetings and setting are important to both enterprises, but they are not without risk:
forging these new alliances  with technologists, scientists, and medical educators  offers the possibility of staking a claim on a territory that has been previously off-limits to the nonscientist cultural theorists. as with other political struggles, the project of alliance building is not without its risks and dangers.  [balsamo 2000, 268]

59
another similarity is instrumentalistic expectations from the "outside." in the case of an institution such as lcc there are expectations of delivering "high culture" and presumably, useful knowledge, to engineering students. at the same time there are basic values and critical perspectives that need to be expressed:
as a feminist scholar, i certainly dont want to abandon the epistemological critique of the construction of scientific knowledge as patriarchal knowledge. nor do i want to give up on the pursuit of social justice through scientific and technological means. this becomes another occasion for the practice of identity-switching  this time not simply between the humanist and the critic, but between the teacher and the advocate. whereas the teacher demands the students engage the philosophical critique of an epistemological worldview and construct their own assessment of the value-laden nature of a particular scientific worldview, the advocate continues to guide them towards careers in science and technology and encourage them to find a way to make a difference.  [balsamo 2000, 271]

60
both balsamos engaging narrative and the narratives of humanities computing speak about being in between, having multiple identities, lacking a stable identity, and engaging richly but not unproblematically with other disciplines within and without the local setting. there is energy, risk-taking and wanting to make a difference in such narratives.
61
georgia tech and traditional humanities computing clearly represent very different approaches to digital humanities. for example, while balsamo sees information technology as a cultural object in need of exploration and epistemological critique, traditional humanities computing treats technology in a more formal and instrumental way. in the next article in this series, an attempt to lay out a more detailed and comprehensive map of the digital humanities will be made. a number of diverse initiatives and approaches are used as examples, and different modes of engagement with the "digital" are discussed at more length. the story of the digital humanities continues to be complex in terms of the theoretical, practice-based, historical, technical and disciplinary foundations and a fast-changing landscape. it is exactly these qualities that make digital humanities an exciting field to study, and a place full of energy and multiple identities.what is humanities computing and what is not?

we are the mimics. clouds are pedagogues. (wallace stevens, notes toward a supreme fiction.[1])

any intelligent entity that wishes to reason about its world encounters an important, inescapable fact: reasoning is a process that goes on internally, while most things it wishes to reason about exist only externally.[2]

abstract

i'll give the short answer to the question 'what is humanities computing?' up front: it is foreshadowed by my two epigraphs. humanities computing is a practice of representation, a form of modeling or, as wallace stevens has it, mimicry. it is also (as davis and his co-authors put it) a way of reasoning and a set of ontological commitments, and its representational practice is shaped by the need for efficient computation on the one hand, and for human communication on the other. we'll come back to these ideas, but before we do, let's stop for a moment to consider why one would ask a question such as 'what is humanities computing?'

first, i think the question arises because it is important to distinguish a tool from the various uses that can be made of it, if for no other reason than to evaluate the effectiveness of the tool for different purposes. a hammer is very good nail-driver, not such a good screw-driver, a fairly effective weapon, and a lousy musical instrument. because the computer is much more than the hammer a general-purpose machine (in fact, a general-purpose modeling machine) it tends to blur distinctions among the different activities it enables. are we word-processing or doing email? are we doing research or shopping? are we entertaining ourselves or working? it's all data: isn't it all just data processing? sure it is, and no it isn't. the goals, rhetoric, consequences, benefits, of the various things we do with computers are not the same, in spite of the hegemony of windows and the web. all our activities may all look the same, and they may all take place in the same interface, the same discourse universe of icons, menus, and behaviors, but they're not all equally valuable, they don't all work on the same assumptions they're not, in fact, interchangeable. to put a more narrowly academic focus on all this, i would hazard a guess that everyone reading this uses a word-processor and email as basic tools of the profession, and i expect that many readers are also in the humanities. even so, you do not all do humanities computing nor should you, for heaven's sake any more than you should all be medievalists, or modernists, or linguists.

so, one of the many things you can do with computers is something that i would call humanities computing, in which the computer is used as tool for modeling humanities data and our understanding of it, and that activity is entirely distinct from using the computer when it models the typewriter, or the telephone, or the phonograph, or any of the many other things it can be.

the second reason one might ask the question 'what is humanities computing' is in order to distinguish between exemplars of that activity and charlatans (c.f. tito orlandi) or pretenders to it. charlatans are, in professor orlandi's view, people who present as 'humanities computing' some body of work that is not that. it may be computer-based (for example, it may be published on the web), and it may present very engaging content, but if it doesn't have a way to be wrong, if one can't say whether it does or doesn't work, whether it is or isn't internally consistent and logically coherent, then it's something other than humanities computing. the problem with charlatanism is that it undersells the market by providing a quick-and-dirty simulacrum of something that, done right, is expensive, time-consuming, and difficult. put another way, charlatans trade intellectual self-consistency and internal logical coherence (in what probably ought to be a massive and complicated act of representation) for surface effects, immediate production, and canned conclusions. when one does this, one is competing unfairly with projects that are more thorough and thoughtful, both in their approach to the problem of representation and in their planning and testing of technical and intellectual infrastructure.

the bad news here is that all humanities computing projects today are involved in some degree of charlatanism, even the best of them. but degree matters, and one way in which that degree can be measured is by the interactivity offered to users who wish to frame their own research questions. if there is none offered, and no interactivity, then the project is probably pure charlatanism. if it offers some (say, keyword searching), then it can be taken a bit more seriously. if it offers structured searching, a bit more so. if it offers combinatorial queries, more so. if it allows you to change parameters and values in order to produce new models, it starts to look very much like something that must be built on a thoroughgoing representation. if it lets you introduce new algorithms for calculating the outcomes of changed parameters and values, then it is extremely well designed indeed. and so on. this evaluative scale is not, as it seems to be, based on functional characteristics: it uses those functional characteristics as an index to the infrastructure that is required to support certain kinds of functionality. on this scale of relative charlatanism, no perfectly exemplary project exists, as far as i know. but you see the principle implied by this scale the more room a resource offers for the exercise of independent imagination and curiosity, the more substantially well thought-out, well-designed, and well-produced a resource it must be.

finally, and most candidly, one asks the question 'what is humanities computing' in order to justify, on the basis of distinctions like those i have just drawn, new and continuing investments of personal, professional, institutional, and cultural resources. this investment could take the form of a funded project, or a new undergraduate or graduate degree, or a new center or institute. at this level, the activity that is humanities computing competes with other intellectual pursuits history, literary study, religious study, etc. for the hearts, minds, and purses of the university, and external funding agencies, even though, in practice, the particulars of humanities computing may well and will likely call upon and fall into one of its competitors' traditional disciplinary areas of expertise. so, as willard mccarty has often noted, we have a problem distinguishing between computing in the service of a research agenda framed by the traditional parameters of the humanities, or, on the other hand, the much rarer, more peculiar case where the humanities research agenda itself is framed and formed by what we can do with computers.

so, given that humanities computing isn't general-purpose academic computing isn't word-processing, email, web-browsing what is it, and how do you know when you're doing it, or when you might need to learn how to do it? at the opening of this discussion, i said that

[h]umanities computing is a practice of representation, a form of modeling or [...] mimicry. it is[...] a way of reasoning and a set of ontological commitments, and its representational practice is shaped by the need for efficient computation on the one hand, and for human communication on the other.[3]

i've long believed this, but the terms of these assertions are drawn from davis, shrobe, and szolovits, what is a knowledge representation? in a 1993 issue of ai magazine. as i unpack these terms, one at a time, i will begin by expanding my quotation of davis et al. a little bit, stopping on each of six points to look at some examples from the realm of humanities computing, and concluding with some observations about why all of this matters.

i. humanities computing as model or mimicry

davis et al. use the term 'surrogate' instead of 'mimicry' or 'model'. here's what they say about surrogates:

the first question about any surrogate is its intended identity: what is it a surrogate for? there must be some form of correspondence specified between the surrogate and its intended referent in the world; the correspondence is the semantics for the representation. the second question is fidelity: how close is the surrogate to the real thing? what attributes of the original does it capture and make explicit, and which does it omit? perfect fidelity is in general impossible, both in practice and in principle. it is impossible in principle because any thing other than the thing itself is necessarily different from the thing itself (in location if nothing else). put the other way around, the only completely accurate representation of an object is the object itself. all other representations are inaccurate; they inevitably contain simplifying assumptions and possibly artifacts.[4]

i.1 example

a catalogue record (vs. full-text representation). the catalogue record is obviously not the thing it refers to: it is, nonetheless, a certain kind of surrogate, and it captures and makes explicit certain attributes of the original object title, author, publication date, number of pages, topical reference. it obviously omits others the full text of the book, for example. now, other types of surrogates would capture those features (a full-text transcription, for example) but would leave out still other aspects (illustrations, cover art, binding). you can go on pushing that as far as you like, or until you come up with a surrogate that is only distinguished from the original by not occupying the same space, but the point is all of these surrogates along the way are 'inaccurate; they inevitably contain simplifying assumptions and possibly artifacts'[5] meaning new features introduced by the process of creating the representation. humanities computing, as a practice of knowledge representation, grapples with this realization that its representations are surrogates in a very self-conscious way, more self-conscious, i would say, than we generally are in the humanities when we represent the objects of our attention in essays, books, and lectures.

ii. humanities computing as a way of reasoning

actually, what davis et al. say is that any knowledge representation is a 'fragmentary theory of intelligent reasoning,'[6] and any knowledge representation begins with.

[...] some insight indicating how people reason intelligently, or [...] some belief about what it means to reason intelligently at all [..] a representation's theory of intelligent reasoning is often implicit, but can be made more evident by examining its three components: (i) the representation's fundamental conception of intelligent inference; (ii) the set of inferences the representation sanctions; and (iii) the set of inferences it recommends. where the sanctioned inferences indicate what can be inferred at all, the recommended inferences are concerned with what should be inferred. (guidance is needed because the set of sanctioned inferences is typically far too large to be used indiscriminantly.) where the ontology we examined earlier tells us how to see, the recommended inferences suggest how to reason. these components can also be seen as the representation's answers to three corresponding fundamental questions: (i) what does it mean to reason intelligently? (ii) what can we infer from what we know? and (iii) what ought we to infer from what we know? answers to these questions are at the heart of a representation's spirit and mindset; knowing its position on these issues tells us a great deal about it.[7]

later on, the authors quote a foundational paper by marvin minsky, setting forth the frame theory. minsky explains:

whenever one encounters a new situation (or makes a substantial change in one's viewpoint), he selects from memory a structure called a frame; a remembered framework to be adapted to fit reality by changing details as necessary. a frame [...] [represents] a stereotyped situation, like being in a certain kind of living room, or going to a child's birthday party.[8]

and they go on to point out, in this quotation, how reasoning and representation are intertwined how we think by way of representations.

ii.1 examples

a concordance. (i) the concordance's fundamental conception of intelligent inference? it assumes that verbal patterns in a text are a key to the meaning of that text. (ii) the set of inferences the concordance sanctions? it would support certain kinds of stylistic analysis, because it can report the frequency with which certain words are used in a text, or the frequency with which words of a certain length are used in a text, and it would support the inference that some words are not important, assuming it can use a stop-list, and if it incorporated a lemmatiser, it would support the notion that word-stems are more important than actual word forms, but (iii) the set of inferences it recommends? most concordancing software makes sorting by frequency and examination of keywords in context much easier than other functions (or forms of inference).

a relational database. think about how a relational database establishes the grounds of rational inference by establishing fields in records in tables, and think about how it sanctions any sort of question having to do with any combination of the elements in its tables, but actually recommends certain kinds of queries by establishing relationships between elements of different tables.

iii. humanities computing as a set of ontological commitments

on the matter of ontological commitments, davis et al. say:

[s]electing a representation means making a set of ontological commitments. the commitments are in effect a strong pair of glasses that determine what we can see, bringing some part of the world into sharp focus, at the expense of blurring other parts. these commitments and their focusing/blurring effect are not an incidental side effect of a representation choice; they are of the essence: a kr is a set of ontological commitments. it is unavoidably so because of the inevitable imperfections of representations. it is usefully so because judicious selection of commitments provides the opportunity to focus attention on aspects of the world we believe to be relevant.[9]

iii.1 examples

ohco (renear, mylonas, durand: refining our notion of what text really is from 1993 same year as the davis article, though to be fair it draws on an earlier piece, s. j. derose, d. g. durand, e. mylonas, and a. h. renear (1990), what is text, really?). this view of text says that text is an ordered hierarchy of content objects, which means, for example, that content objects nest paragraphs occur within chapters, chapters in volumes, and so on. it also means that a language that captures ordered hierarchical relationships and allows content to be carried within its expression of those relationships can capture what matters about text. hence sgml. but, as jerry mcgann and others have pointed out, this view of text misses certain textual ontologies metaphor, for example because they are not hierarchical, or more accurately, they violate hierarchy. davis et al. would say that's not a sign of a flaw in sgml (or xml, which shares the same requirement for nesting) or in the ohco thesis, but a sign that both are true knowledge representations they bring certain things into focus and blur others, allowing us to pay particular attention to particular aspects of what's out there.

deborah parker's dante project: for a much simpler example, consider deborah parker's sgml edition of dante's inferno (<http://www.iath.virginia.edu/dante> (31.10.2002)). in this edition, parker has marked up (in the tei dtd) all of the cantos, stanzas, and lines in dante's poem, and then all of the proper names and epithets, distinguishing mythical, historical, biblical, and literary sources, different types of animals, different types of people, regularizing forms of proper names, etc. all of this implies that the form of the poem is important as a kind of substrate for references to proper names, and that by paying attention to the categories in which named things participate, we can learn something important about this poem.

iv. humanities computing as shaped by the need for efficient computation

davis et al. explain:

from a purely mechanistic view, reasoning in machines (and somewhat more debatably, in people) is a computational process. simply put, to use a representation we must compute with it. as a result, questions about computational efficiency are inevitably central to the notion of representation.[10]

and later, they point out that different modes of representation have different efficiencies:

traditional semantic nets facilitate bi-directional propagation by the simple expedient of providing an appropriate set of links, while rule-based systems facilitate plausible inferences by supplying indices from goals to rules whose conclusion matches (for backward chaining) and from facts to rules whose premise matches (forward chaining).[11]

iv.1 examples

markup and computation. the reason for requiring that elements nest properly within a specified hierarchy is to enable efficient computation. in fact, the sgml grammar in its original form was really too flexible to be efficient, which is why certain features permitted in the grammar (like overlapping or concurrent hierarchies) werefine ver implemented in software. xml simplifies out of sgml some of its other expressive possibilities possibilities that made sgml difficult to write software for and as a result, suddenly we have lots more software for xml than we ever had for sgml. on the other hand, none of this software is any good at computing things that can't be expressed in neatly nesting hierarchies.

latent semantic indexing. compare the characteristics of the concordance, and its efficiencies, with those of latent semantic indexing. like the concordance,

lsi relies on the constituent terms of a document to suggest the document's semantic content. however, the lsi model views the terms in a document as somewhat unreliable indicators of the concepts contained in the document. it assumes that the variability of word choice partially obscures the semantic structure of the document. by reducing the dimensionality of the term-document space, the underlying, semantic relationships between documents are revealed, and much of the noise (differences in word usage, terms that do not help distinguish documents, etc.) is eliminated. lsi statistically analyses the patterns of word usage across the entire document collection, placing documents with similar word usage patterns near each other in the term-document space, and allowing semantically-related documents to be near each other even though they may not share terms' (letsche and barry, large-scale information retrieval with latent semantic indexing[12]).

if you really believed that the occurrence of a particular word was the important thing, then you'd want to be working with the efficiencies of the concordance but if, on the other hand, you believed that meaning was more important than the word chosen to express it, you'd want to be working with the efficiencies of latent semantic indexing.

v. humanities computing as shaped by the need for human communication

davis et al. conclude that any efficiency stands opposed in some way to the fullness of expression, and that

[e]ither end of this spectrum seems problematic: we ignore computational considerations at our peril, but we can also be overly concerned with them, producing representations that are fast but inadequate for real use.[13]

of course, there is something about the brute facticity of the computer that makes its results especially when they are fast seem definitive, so much so that we may overlook the inadequacy of a representation that seems to work well computationally. but eventually, we are likely to recognize inadequacy, and we are more likely to do so if we have not only to use these representations, but also to produce them. on this final point, davis et al. go on to say:

knowledge representations are also the means by which we express things about the world, the medium of expression and communication in which we tell the machine (and perhaps one another) about the world. [...] a medium of expression and communication for use by us. that in turn presents two important sets of questions. one set is familiar: how well does the representation function as a medium of expression? how general is it? how precise? does it provide expressive adequacy? etc. an important question less often discussed is, how well does it function as a medium of communication? that is, how easy is it for us to 'talk' or think in that language? what kinds of things are easily said in the language and what kinds of things are so difficult as to be pragmatically impossible? note that the questions here are of the form 'how easy is it?' rather than 'can we?' this is a language we must use, so things that are possible in principle are useful but insufficient; the real question is one of pragmatic utility. if the representation makes things possible but not easy, then as real users we may never know whether we have misunderstood the representation and just do not know how to use it, or it truly cannot express some things we would like to say. a representation is the language in which we communicate, hence we must be able to speak it without heroic effort.[14]

v.1 example

the difficulty of using markup languages. ever since we started using markup languages like sgml, one has heard expressed the fear that humanists would never be able to speak it 'without heroic effort'. to be fair, good (and with xml, readily available) software removes some of the complexity for example, by offering you only the elements that can legally be used in a particular point in the hierarchy. but still, you have to be able to grasp the purpose and intent of the dtd in order to use it sensibly, you have to understand the principles of stylesheets, and so on. it would probably be accurate, at this moment in the evolution of humanities computing, to say that markup languages are still problematic as a medium of communication. experts can talk or think in these languages, but most of us cannot, and there are many examples out there, in discussions on tei-l (the tei users list) for example, where the question at issue is exactly whether one has misunderstood the tei or whether it really cannot express some of the things we would like to say about literary and linguistic texts.

vi. humanities computing and formal expression

there is also one other feature of knowledge representations that davis and his co-authors don't mention, because their discussion takes it for granted. that feature is the formal language in which any such representation must be expressed. this formal language can be any one that is

composed of primitive symbols acted on by certain rules of formation (statements concerning the symbols, functions, and sentences allowable in the system) and developed by inference from a set of axioms. the system thus consists of any number of formulas built up through finite combinations of the primitive symbols combinations that are formed from the axioms in accordance with the stated rules.[15]

for our purposes, what is important about the requirement of formal expression is that it puts humanities computing, or rather the computing humanist, in the position of having to do two things that mostly, in the humanities, we don't do: provide unambiguous expressions of ideas, and provide them according to stated rules. in short, once we begin to express our understanding of, say, a literary text in a language such as xml, a formal grammar that requires us to state the rules according to which we will deploy that grammar in a text or texts, then we find that our representation of the text is subject to verification for internal consistency, and especially for consistency with the rules we have stated.

conclusions

having said what i think humanities computing is, it remains to say what it is good for, or why it matters. why do we need to worry about whether we can express what we know about the humanities in formal language, in terms that are tractable to computation, in utterances that are internally coherent and consistent with a declared set of rules? why indeed, when we know that to do this inevitably involves some loss of expressive power, some tradeoff at the expense of nuance, meaning, and significance? my answer? navigation and exchange.

we are by now well into a phase of civilization when the terrain to be mapped, explored, and annexed is information space, and what's mapped is not continents, regions, or acres but disciplines, ontologies, and concepts. we need representations in order to navigate this new world, and those representations need to be computable, because the computer mediates our access to this world, and those representations need to be produced at first-hand, by someone who knows the terrain. if, where the humanities should be represented, we in the humanities scrawl, or allow others to scrawl, 'here be dragons', then we will have failed. we should not refuse to engage in representation simply because we feel no representation can do justice to all that we know or feel about our territory. that's too fastidious. we ought to understand that maps are always schematic and simplified, but those qualities are what make them useful.

in some form, the semantic web is our future, and it will require formal representations of the human record. those representations ontologies, schemas, knowledge representations, call them what you will should be produced by people trained in the humanities. producing them is a discipline that requires training in the humanities, but also in elements of mathematics, logic, engineering, and computer science. up to now, most of the people who have this mix of skills have been self-made, but as we become serious about making the known world computable, we will need to train such people deliberately. there is a great deal of work for such people to do not all of it technical, by any means. much of this map-making will be social work, consensus-building, compromise. but even that will need to be done by people who know how consensus can be enabled and embodied in a computational medium.

consensus-based ontologies (in history, music, archaeology, architecture, literature, etc.) will be necessary, in a computational medium, if we hope to be able to travel across the borders of particular collections, institutions, languages, nations, in order to exchange ideas. those ontologies will in turn exist in a network of topics, a web of trading zones, to use a term that willard mccarty has used to explain humanities computing, having borrowed that term from a book that itself borrows concepts of anthropology to explain the practice of physics. and as that genealogy of that metaphor suggests, come tomorrow, we will require the rigor of computational methods in the discipline of the humanities not in spite of, but because of, the way that human understanding and human creativity violate containment, exceed representation, and muddle distinctions.  what is digital humanities and what's it doing in english departments?
people who say that the last battles of the computer revolution in english departments have been fought and won don't know what they're talking about. if our current use of computers in english studies is marked by any common theme at all, it is experimentation at the most basic level. as a profession, we are just learning how to live with computers, just beginning to integrate these machines effectively into writing and reading-intensive courses, just starting to consider the implications of the multilayered literacy associated with computers.
'cynthia selfe
what is (or are) the 'digital humanities,' aka 'humanities computing'? it's tempting to say that whoever asks the question has not gone looking very hard for an answer. 'what is digital humanities?' essays like this one are already genre pieces. willard mccarty has been contributing papers on the subject for years (a monograph too). under the earlier appellation, john unsworth has advised us 'what is humanities computing and what is not.' most recently patrik svensson has been publishing a series of well-documented articles on multiple aspects of the topic, including the lexical shift from humanities computing to digital humanities. moreover, as cynthia selfe in an ade bulletin from 1988 reminds us, computers have been part of our disciplinary lives for well over two decades now. during this time digital humanities has accumulated a robust professional apparatus that is probably more rooted in english than any other departmental home.
 e contours of this professional apparatus are easily discoverable. an organization called the alliance of digital humanities organizations hosts a well-attended annual international conference called digital humanities (it grew out of an earlier annual series of conferences, hosted jointly by the association for computers and the humanities and the association for literary and linguistic computing since 1989).  ere is blackwell's companion to digital humanities.  ere is a book series (yes, a book series), topics in the digital humanities, from the university of illinois press.  ere is a refereed journal called digital humanities quarterly, one of several that serve the field, including a newer publication, digital studies / le champ numerique, sponsored by the canadian society for digital humanities (societe pour l'etude des medias interactifs).  e university of victoria hosts the annual digital humanities summer institute to train new scholars. crucially, there are digital humanities centers and institutes (probably at least one hundred worldwide, some of them established for a decade or more with staffs numbering in the dozens): these are served by an organization known as centernet.  ere have been digital humanities manifestos (i know of at least two) and faqs, colloquia and symposia, workshops and special sessions. not to mention, of course, that a gloss or explanation of digital humanities is implicit in every mission statement, every call for papers and proposals, every
  strategic plan and curriculum-development document, every hiring request, and so forth that invokes the term. or the countless times the question has been visited on electronic discussion lists, blogs, facebook walls, and twitter feeds, contributing all the  ames and exhortations, celebrations and screeds one could wish to read.
we could also, of course, simply google the question. google takes us to wikipedia, and what we find there is not bad:
 e digital humanities, also known as humanities computing, is a field of study, research, teaching, and invention concerned with the intersection of computing and the disciplines of the humanities. it is methodological by nature and interdisciplinary in scope. it involves investigation, analysis, synthesis and presentation of information in electronic form. it studies how these media affect  the disciplines in which they are used, and what these disciplines have to contribute to our knowledge of computing.
as a working definition this serves as well as any i've seen, which is not surprising since a glance at the page's view history tab reveals individuals closely associated with the digital humanities as contributors. at its core, then, digital humanities is more akin to a common methodological outlook than an investment in any one specific  set of texts or even technologies. we could attempt to refine  this 'outlook' quantitatively, using some of the very tools and techniques digital humanities has pioneered. for example, we might use a text-analysis tool named voyeur developed by stefan sinclair to mine the proceedings from the annual digital humanities conference and develop lists of topic frequencies or collocate key terms or visualize the papers' citation networks. we could also choose to explore the question qualitatively, by examining sets of projects from self-identified  digital humanities centers. at the university of maryland, where i serve as an associate director at the maryland institute for technology in the humanities, we support work from 'shakespeare to second life' as we're fond of saying: the shakespeare quartos archive, funded by a joint grant program administered by the united kingdom's jisc and the neh, makes a searchable digital facsimile of each of the thirty-two extant quarto copies of hamlet available online, while the preserving virtual worlds project, supported by the library of congress, has developed and tested standards and best practices for archiving and ensuring future access to computer games, interactive fiction, and virtual communities.
yet digital humanities is also a social undertaking. it harbors networks of people who have been working together, sharing research, arguing, competing, and collaborating for many years. key achievements from this community, like the text encoding initiative or the orlando project, were mostly finished  before the current wave of interest in digital humanities began. nonetheless, the rapid and remarkable rise of digital humanities as a term can be traced to a set of surprisingly specific  circumstances. unsworth, who was the founding director of the institute for advanced technology in the humanities at the university of virginia for a decade and is currently dean of the graduate school of library and information science at the university of illinois, has this to relate:
 e real origin of that term [digital humanities] was in conversation with andrew mcneillie, the original acquiring editor for the blackwell companion to digital humanities. we started talking with him about that book project in 2001, in april, and
   by the end of november we'd lined up contributors and were discussing the title, for the contract. ray [siemens] wanted 'a companion to humanities computing' as that was the term commonly used at that point; the editorial and marketing folks at blackwell wanted 'companion to digitized humanities.' i suggested 'companion to digital humanities' to shift the emphasis away from simple digitization. (message)
at about the same time the blackwell's volume was being put together, the leadership of two scholarly organizations opened discussions about creating an umbrella entity for themselves and eventually other organizations and associations with like interests. as anyone who has ever tried to run a scholarly organization will know, economies of scale are difficult to come by with only a few hundred members and so the thought was to consolidate and share infrastructure and services.  e two organizations were the aforementioned association for computers in the humanities and the association for literary and linguistic computing.  e umbrella structure that resulted was called adho, or the alliance of digital humanities organizations. here is unsworth again, from the same communication:
conversations about merging ach and allc began at tuebingen, in a bar, in a conversation between harold short and me, in july 2002. a couple of months later, i had set a list called 'adhoc''allied digital humanities organizations committee), first message dated august 16, 2002. . . . we finally got things o  the dime in sweden, at the 2004 allc/ach, and after waffling some more about names (ichio, ohco, and others) we voted, in april of 2005, to go with adho, changing 'a' from 'allied' to 'alliance.'
by 2005 then, the blackwell's companion had been published and the alliance for digital humanities organizations had been established.  ere's one more key event to relate, and that's the launch, in 2006, of the digital humanities initiative by the neh, then under the chairmanship of bruce cole and with leadership provided by brett bobley, a charismatic and imaginative individual who doubles as the agency's cio. in an e-mail to me, bobley describes a january 2006 lunch with another neh staffer at which they were brainstorming ideas for what would become the digital humanities initiative:
at the lunch, i jotted down a bunch of names, including humanities computing, ehumanities, and digital humanities. when i got back to the office, i googled all three of them and 'digital humanities' seemed to be the winner. i liked it for a few reasons: due to adho and their annual digital humanities conference, the name brought up a lot of relevant hits. i believe i'd also heard from julia flanders about the forthcoming digital humanities quarterly journal. i also appreciated the fact that it seemed to cast a wider net than 'humanities computing' which seemed to imply a form of computing, whereas 'digital humanities' implied a form of humanism. i also thought it would be an easier sell to the humanities community to have the emphasis on 'humanities.'
in 2008 the digital humanities initiative became the office of digital humanities, the designation of 'office' assigning the program (and its budget line) a permanent place within the agency.  at the major federal granting agency for scholarship in the humanities, taking its cues directly from a small but active and influential group of scholars, had devoted scarce resources to launching a number of new grant
  opportunities, many of them programmatically innovative in and of themselves, around an endeavor termed 'digital humanities' was doubtless the tipping point for the branding of dh, at least in the united states.
 these events will, i think, earn a place in histories of the profession alongside other major critical movements like the birmingham school or yale deconstruction. in the space of a little more than five years digital humanities had gone from being a term of convenience used by a group of researchers who had already been working together for years to something like a movement. individual scholars routinely now self-identify as digital humanists, or 'dhers.'  ere is an unusually strong sense of community and common purpose, manifested, for example, in events such as the day of digital humanities, organized by a team at the university of alberta. its second annual iteration featured over 150 participants (up from around one hundred the first year), who blogged on a shared site about the details of their workday, posted photographs of their offices and screens, and reflected on the nature of their enterprise. digital humanities has even been the recipient of its own downfall remix, the internet meme whereby the climactic scene from the hbo  lm depicting hitler's final days in the bunker is closed-captioned with, in this instance, a tirade about the pernicious influence of online scholarship.
digital humanities was also (you may have heard) big news at the 2009 mla annual convention in philadelphia. on 28 december, midway through the convention, william pannapacker, one of the chronicle of higher education's officially appointed bloggers, wrote the following for the online 'brainstorm' section: 'amid all the doom and gloom of the 2009 mla convention, one field seems to be alive and well: the digital humanities. more than that: among all the contending subfields, the digital humanities seem like the first 'next big thing' in a long time.' (it seems fair to say that pannapacker, who is the author of 'graduate school in the humanities: just don't go,' under the pseudonym  omas benton, is not a man easily impressed.) jennifer howard, meanwhile, a veteran chronicle reporter who has covered the convention before, noted the 'vitality' of digital humanities with its 'over ow crowds to too-small conference rooms.'  ere were several dozen panels devoted to the digital humanities at the mla convention, and one could (and did) easily navigate the three-day convention by moving among them.
crucially, digital humanities was visible in another way at the conference: the social-networking service twitter. twitter is the love-it-or-hate-it web 2.0 application often maligned as the final triumph of the attention-de cit generation because it limits postings to a mere 140 characters'not 140 words, 140 characters.  e reason has less to do with attention spans than twitter's origins in the messaging protocols of mobile devices, but the format encourages brief, conversational posts ('tweets') that also tend to contain a fair measure of  air and wit. unlike facebook, twitter allows for asymmetrical relationships: you can 'follow' someone (or they can follow you) without the relationship's being reciprocated. tweeting has rapidly become an integral part of the conference scene, with a subset of attendees on twitter providing realtime running commentary through a common 'tag' (#mla09, for example), which allows everyone who follows it to tune in to the conversation.  is phenomenon has
  some very specific  rami cations. amanda french ran the numbers and concluded that nearly half (48%) of attendees at the digital humanities 2009 conference were tweeting the sessions. by contrast, only 3% of mla convention attendees tweeted' according to french's data, out of about 7,800 attendees at the mla convention only 256 tweeted. of these, the vast majority were people already associated with digital humanities through their existing networks of followers. jennifer howard, again writing for the chronicle, noted the centrality of twitter to the dh crowd and its impact on scholarly communication, going so far as to include people's twitter identities in her roundup of major stories from the convention. inside higher ed also devoted coverage to twitter at the mla convention, noting that rosemary g. feal was using it to connect with individual members of the organization'not surprisingly, many of them dhers. feal, in fact, kept up a lively stream of tweets throughout the conference, gamely mixing it up with the sometimes irreverent back-channel conversation and, in a scene out of small world had it only been written twenty years later, issued an impromptu invite for her 'tweeps' to join the association's elite for nightcaps in the penthouse of one of the convention hotels.
while it's not hard to see why the academic press devoured the story, there's more going on than mere shenanigans. twitter, along with blogs and other online outlets, has inscribed the digital humanities as a network topology, that is to say, lines drawn by aggregates of affinities, formally and functionally manifest in who follows whom, who friends whom, who tweets whom, and who links to what. digital humanities has also, i would propose, lately been galvanized by a group of younger (or not so young) graduate students, faculty members (both tenure line and contingent), and other academic professionals who now wield the label 'digital humanities' instrumentally amid an increasingly monstrous institutional terrain de ned by declining public support for higher education, rising tuitions, shrinking endowments, the proliferation of distance education and the for-pro t university, and, underlying it all, the conversion of full-time, tenure-track academic labor to a part-time adjunct workforce. one example is the remarkable tale of brian croxall, the recent emory phd who went viral online for a period of several weeks during and after the mla. croxall had his paper, ' e absent presence: today's faculty,' read at the convention in absentia while he simultaneously published it on his blog after  nding himself unable to a ord to travel to philadelphia because he hadn't landed any convention interviews. as numerous observers pointed out, croxall's paper, which was heavily blogged and tweeted and received coverage in both the chronicle and inside higher ed, was undoubtedly and by many orders of magnitude the most widely seen and read paper from the 2009 mla convention.  ese events were subsequently discussed in a series of cross-postings and conversations that spilled across twitter and the blogosphere for several weeks after the convention ended. many seemed to feel that the connection to wider academic issues was not incidental or accidental, and that digital humanities, with a culture that values collaboration, openness, nonhierarchical relations, and agility might be an instrument for real resistance or reform.
so what is digital humanities and what is it doing in english departments?  e answer to the latter portion of the question is easier. i can think of some half a dozen reasons why english departments have historically been hospitable settings for this
  kind of work. first, after numeric input, text has been by far the most tractable data type for computers to manipulate. unlike images, audio, video, and so on, there is a long tradition of text-based data processing that was within the capabilities of even some of the earliest computer systems and that has for decades fed research in fields like stylistics, linguistics, and author attribution studies, all heavily associated with english departments. second, of course, there is the long association between computers and composition, almost as long and just as rich in its lineage.  ird is the pitch-perfect convergence between the intense conversations around editorial theory and method in the 1980s and the widespread means to implement electronic archives and editions very soon after; jerome mcgann is a key figure here, with his work on the rossetti archive, which he has repeatedly described as a vehicle for applied theory, standing as paradigmatic. fourth, and at roughly the same time, is a modest but much-promoted belle-lettristic project around hypertext and other forms of electronic literature that continues to this day and is increasingly vibrant and diverse. fifth is the openness of english departments to cultural studies, where computers and other objects of digital material culture become the centerpiece of analysis. i'm thinking here, for example, of the reader stuart hall and others put together around the sony walkman, that hipster ipod of old. finally, today, we see the simultaneous explosion of interest in e-reading and e-book devices like the kindle, ipad, and nook and the advent of large-scale text digitization projects, the most significant of course being google books, with scholars like franco moretti taking up data mining and visualization to perform 'distance readings' of hundreds, thousands, or even millions of books at a time.
digital humanities, which began as a term of consensus among a relatively small group of researchers, is now backed on a growing number of campuses by a level of funding, infrastructure, and administrative commitments that would have been unthinkable even a decade ago. even more recently, i would argue, the network effects of blogs and twitter at a moment when the academy itself is facing massive and often wrenching changes linked both to new technologies and the changing political and economic landscape has led to the construction of 'digital humanities' as a free floating signifier, one that increasingly serves to focus the anxiety and even outrage of individual scholars over their own lack of agency amid the turmoil in their institutions and profession.  is is manifested in the intensity of debates around openaccess publishing, where faculty members increasingly demand the right to retain ownership of their own scholarship'meaning, their own labor'and disseminate it freely to an audience apart from or parallel with more traditional structures of academic publishing, which in turn are perceived as outgrowths of dysfunctional and outmoded practices surrounding peer review, tenure, and promotion (see fitzpatrick on 'planned obsolescence' in this issue).
whatever else it might be then, the digital humanities today is about a scholarship (and a pedagogy) that is publicly visible in ways to which we are generally unaccustomed, a scholarship and pedagogy that are bound up with infrastructure in ways that are deeper and more explicit than we are generally accustomed to, a scholarship and pedagogy that are collaborative and depend on networks of people and that live an active 24/7 life online. isn't that something you want in your english department?information technology
and the troubled humanities

where will information technology leave humanities education five, ten, twenty. . .years from now. this essay addresses that question in the context of several current 'crises' facing humanities scholarship and education. these crises have followed from the displacement of traditional philological work from the center of the literary and cultural studies' curriculum'in particular editorial theory and method, history of the language, and bibliographical studies. the coming of digital technology to the humanities has revealed the historical necessity of recovering these basic disciplinary skills.
keywords: humanities computing, theory of textuality, scholarly publishing, theory and method of interpretation, research libraries.
1.
let me begin with henry adams, whose urbane pessimism gets summarized in this late passage from his famous autobiography:
he saw his education complete, and was sorry he ever began it. as a matter of taste, he greatly preferred his eighteenth-century education when god was a father and nature a mother, and all was for the best in a scientific universe. he repudiated all share in the world as it was to be and yet he could not detect where his responsibility began or ended.
henry adams, the education of henry adams, chapter 31 (1907).
an education ought to make one ready for life, but adams' education has turned out a kind of black comedy. his humanistic training has
left him unprepared for the dynamo of the twentieth-century. so he joins the coming race as an observer, a scholar what he calls an 'historian'. but 'all that the historian won was a vehement wish to escape'.
today, as we pass through a similar historical moment, a moment even more wrenching for a humanist than adams' moment, the education seems especially pertinent. we don't want to guide our passage through this moment with tabloid reports like the gutenberg elegies, which supply us with a cartoon set of alternatives. information technology comprises an axis of evil that birkerts advises us to 'refuse'. we can no more 'refuse' this digital environment than we can 'refuse' the empire my country, for better and for worse, has become. we may well feel 'a violent wish to escape' both of these unfolding'and closely enfolded'histories, but we do better to recall that as we are characters in these events, we bear a responsibility toward them.
and there precisely we find henry adams waiting for us, caught between two worlds. not between a dead world and a world powerless to be born, however, but between two living worlds, one relatively young, the other ancient. he neither abandons the one nor refuses the other. the positive revelation of his great book tells us that we all always inhabit such a condition. at certain historical moments, that universal experience seems especially clear, and certain figures come forward to render an honest accounting.
his book also tells a cautionary tale, which is the second gift it passes on to us. if the dynamo and the virgin each have their humanities in adams' view, he represents himself as the nowhere man. not that he takes no action, but that he restricts his action to honest reporting. as a consequence, both virgin and dynamo emerge from his book as mysterious forces'in fact, as those 'images' which so preoccupy and immobilize him throughout his book.
'where will information technology leave humanities education five/ten/twenty/. . .n years from now?' the question implicitly asks for something more than an honest report. reading adams helps me remember to be wary of making forecasts. but he also reminds me that i do have hopes, as well as a few convictions about how we should look at those futures we imagine lying ahead of us.
so let me begin with a conviction: that we have to carry out what marxist scholars used to call 'the praxis of theory''or as the poet better said, we must learn by going where we have to go. involved here are two hard sayings that can no longer be fudged or tabled. first, integrating digital technology into our scholarship will have to be pursued on as broad a scale as possible. circumstances are such that this work can no longer be safely postponed. second, we have to restore textual and bibliographical work to the center of what we do.
'what are you saying? learn unix, hypermedia design, one or more programming languages, or textual markup and its discontents? learn bibliography and the sociology of texts, ancient and modern textual theory, history of the book?' yes, that is exactly what i am saying.' and of course you ask why. at this point i give only one reason, though by itself'if we draw out its implications'the reason will more than suffice: because information technology is even now transforming the fundamental character of the library. the library, the chief locus of our cultural memory as well as our central symbol of that memory's life and importance. that transformation is already altering the geography of scholarship, criticism, and educational method throughout the humanities and it forecasts even more dramatic changes ahead, as i shall indicate later. moreover , the shifting plates are already registering on the seismographs.
let's begin at that point, with the signals coming from current, well-known events. first of all, some happy signs of the times. already the library's reference rooms are well along to virtually complete virtualization, and it's difficult to believe any scholar regrets this. the transformation reflects the relative ease with which expository and informational materials translate into digital forms. to have immediately available to you those resources, wherever you might choose to set up your computer and go online, is a clear gain, and for older persons, an amazement. such things can turn the soberest scholar into a digital groupie. young persons tend to take such marvels for granted.
and it's also the case that some remarkable scholars have been acute to see the educational opportunities that information technology makes possible for the humanities. the list is distinguished and extensive. but of course it's also the case that, compared with the distinguished and extensive body of paper-based scholarship, this list of it projects is miniscule.
now for some tales from the dark side.
late in the 19th century matthew arnold looked to france as a model for a salutary 'influence of academies' on culture in general. 25 years ago arnold's academic inheritors appeared to be living the realization of his hope. but then came the crash. humanities scholarship and education has been a holy mess for some time. looking at the way we
live now in the academy, one can hardly not recall trollope's dark portrayal of the way we live now. what's going on? where are the snows of yesteryear?
something like those very questions drove the editor of critical inquiry, w. j. t. mitchell, to summon the journal's board of editors to a symposium in april 2003 'to discuss the future of the journal and of the interdisciplinary fields of criticism and theory' (324). some of the most distinguished north american academics gathered in chicago to assess the bad eminence that higher education in the humanities has gained' specific ally, to ponder 'the future of criticism' and in particular of critical theory. i missed the friday night public forum and pep-rally for the symposium but made it for the key event, the day-long saturday discussions. from these i went home shocked and more than a little dismayed by what i learned.
most of us registered, one way or another, the malaise that has grown widespread in the humanities in america, and i wasn't particularly disheartened that we were all uncertain about how best to deal with the problems we talked about. something else was troubling, however: the degree of ignorance about information technology and its critical relevance to humanities education and scholarship. i've spent almost twenty years studying this subject in the only way you have a chance of doing something useful. that is, by hands-on collaborative interdisciplinary work. by designing and building the materials and applications tools that alone can teach how best to make and use these things. you don't learn a language by talking about it or reading books. you learn it by speaking it and writing it. there's no other way. anything less is just, well, theoretical.
so far as information technology concerns traditional humanities, the issues are more clearly understood in the united kingdom and europe than they are in the united states. moreover, if you want to engage serious, practical conversation about humanities education and digital culture, america's most famous humanities research institutions'with few exceptions'are not the places to go.
the ci meeting explained why. we're illiterate. besides myself, no one on the ci board can use any of the languages we need to understand how to operate with our proliferating digital technologies' not even elementary markup languages. most had never heard of tei and no one i talked with was aware of the impact it was already exerting on humanities scholarship and education. the library, especially the research library,
is a cornerstone if not the very foundation of modern humanities. it is undergoing right now a complete digital transformation. in the coming decades'the process has already begun'the entirety of our cultural inheritance will be transformed and re-edited in digital forms. do we understand what that means, what problems it brings, how they might be addressed? theoretical as well as very practical discussions about these matters have been going on for years and decisions are taken every day. yet digital illiteracy puts many of us on the margin of conversations and actions that affect the center of our cultural interests (as citizens) and our professional interests (as scholars and educators).
this situation has to change, and in the last part of this essay i will briefly  describe a project called nines that would if successful help the change along. the project is practical in four ways: it addresses some of the most basic needs and self-interests of the working humanities scholar; it focuses on a limited, controllable region of the humanities (nineteenthcentury literary and cultural studies in britain and america); it involves a collaboration between three key institutional entities (the research library, the individual scholar, and the professional organizations that help us to integrate and organize our work); and it has been designed for adaptation and scalabaility to other disciplinary areas.
what seems to me impractical is to continue framing the crisis in humanities scholarship and education in the theological terms of 'critical theory' and 'cultural studies'. the public glances at goings-on like the critical inquiry symposium with ironic amusement. to the reporters from new york and boston who covered the symposium, it recalled nothing so much as chaucer's parliament of foules, as we know from the stories they  led. and every year, as we also know, the mla's annual meeting provides the media with comic relief.
but our tight little island's problems are by no means trivial, nor are they removed from the larger social scene. we have obligations as we are educators, obligations that society expects us to meet because of our special humanist vocation.
remember that marxian distinction between the base and the superstructure? remember it. our ideological conflicts today are deeply imbedded'commercially, economically, institutionally. because this is the case, we have useful, practical things we can and should be doing. but before those doings become possible, we shall first have to stop the cant pervading so much of our discourse. an especially dismal aspect of our professional writing today is its ineffectual angelism, which is widespread.
this problem is not simply a matter of prose styles, or their failure. our publications ride high on jargons of moral, social, and political engagement. in truth, these styles largely measure the extremity of our intramural focus and social disengagement. to be 'transgressive' in a routledge book or a critical inquiry essay'that word 'transgressive' has grown legions'is simply disspiriting. jargons of impiety and critique'our current rhetorics of displacement'define the treason of the intellectuals, the signs of a transgression that has no referent, not even an intramural one. writing for tenure committees and an overhearing professoriat, we mistake shop talk for scholarship and criticism. the worst of it, for the humanities scholar anyhow, is the abuse we inflict on the language we are missioned to preserve and protect.
to begin with such a practical self-criticism would make a real difference in the way we do our work. but humanities scholars face another set of problems and obligations'perhaps even more serious, certainly much less tractable. to expose them clearly let me revisit the way we live now from a slightly different perspective. let us set our inner standingpoint at the level of the base this time, not the superstructure.
next to critical inquiry's apprehensions about the state of critical theory we should reflect on stephen greenblatt's pragmatic worry about 'the crisis in tenure and publishing'. in a special letter to the members of the modern language association in may 2002, greenblatt'then mla president'pointed to dire academic publishing conditions. he called the problem, correctly, a 'systemic' one. a network of relations has bound together for a long time the work of scholarship, academic appointment, and paper-based'in particular, university press'publishing. this network has been breaking up, or down, for many years, and the pace of its unraveling only accelerates.
the problem is that most university presses are running at increasingly sharp de cits. this trend will not be reversed, as everyone inside the university publishing network knows. we produce larger and larger amounts of scholarship and pass it to a delivery system with diminishing capacities to sustain its publication. as an editor of a monograph series, the virginia victorian studies, i have seen how this pressure alters what a university press is prepared to undertake. the notorious stigma that has grown up recently against 'single-author studies' is only one sign of the difficulty. in a grotesque inversion of our most basic goals, near-term economics, not long-term scholarship, has become a serious factor shaping and guiding humanities research for some time. just try to find a publisher
for primary documentary materials, or for any basic research that doesn't come labeled for immediate consumption: 'sell this by such and such a date''before it spoils.
but that is to speak only of book publication. we should be aware that a parallel problem, every bit as acute, exists for periodical publication, where a similar dysfunction can be observed. in each of these cases the university library has become almost the only reliable purchaser of scholarly books and periodicals; and every year, as we know, library funds for such materials get cut further.
the problem has been revisited in the most recent issue of profession, the 'journal of opinion about and for the modern language profession'. but while the four essays in this 'publishing and tenure crises forum' describe the problem quite well, their hopes and proposals, i'm sorry to say, fail to address the 'systemic''the institutional and economic'issues. to imagine that funding infusions from acls, neh, and mellon will stem this tide is to imagine that sandbags will hold back a tsunami.
understand, we're not talking here about 'the death of the book'. as we know, book publishing is alive and well and shows no signs of crisis. the problem is that scholarly communication operates in a highly restricted and specialized market. general publishing, by contrast, is open, with a diverse and dynamically changing audience to which publishers can both appeal and respond. the academic market is largely closed. we are the persons who, all but alone, produce and consume in this market. the academic market used to be somewhat more broadly distributed, but in the past 25 years it has drastically shrunk back upon itself. at the same time, the number of producers'of those who, by systematic and professional demand, are required to produce'has grown enormously. when we then add to the equation the drastic collapse in consumer demand in this market, we are not surprised at the telling numbers. in 1990 a university press would typically print 1000-1500 copies of an academic book. today the number is 200-250 and dropping every year.
many realize that online scholarly publication is the natural and inevitable response to this crisis of scholarly and educational communication. how to bring about the transition to online publication is the $64,000 question. and it's not the technology that makes the problem so difficult, as the examples of online journal publication, jstore (http://www.jstor. org/) and project muse (http://muse.jhu.edu/), demonstrate. the jordan will not be crossed until scholars and educators are prepared not simply
to search and access archived materials online'which is increasingly done'but to publish and to peer-review online'to carry out the major part of our productive educational work in digital forms.
the institutional resistance to such a major change in scholarly work behaviors is widespread, deep, and entirely understandable. it is not in the short-term (immediate) interest of scholars or their institutions to make a transition to digital work. the upfront costs are high, the learning curve is steep. most telling of all, the design of the in-place paper-based system has the sophistication and clear strengths that come from hundreds of years of practical use. with rare exceptions, established scholars have the least practical involvement with information technology. this too is understandable. the known scholar can still, usually, get his or her work published in the usual paper-based ways precisely because they are known, if diminished, quantities.
the consequences of this situation are apparent. for traditional paper-based work, it is 'the crisis in humanities'. for humanists who work with information technology, it is another form of that crisis. digital scholarship'even the best of it'is all more or less atomized, growing like so many topsies. worse, these creatures are idiosyncratically designed and so can't easily talk to each other. they also typically get born into poverty'even the best-funded ones. ensuring their maintenance, development, and survival is a daunting challenge. worst of all, the work regularly passes without much practical institutional notice. the annual mla bibliography still does not cite online works, no matter how distinguished. accepted professional standards do not control the work in objective ways. most of it comes into being without oversight or peerreview.
'what is to be done?' lenin's famous question is very much to the point here, for our scholarship is facing a future that is at once certain and uncertain. it is going to be cast and maintained and disseminated in digital forms. we may not now approve of this but it is nonetheless inevitable. we may not now know how to do this but we will learn. because we have no choice.
2.
before getting to the choice i want to talk about, i have to tell one more academic story. this tale stretches back a bit.
for as long as i've been an educator'since the mid-1960s'a system of apartheid has been in place in literary and cultural studies. on one hand we have editing, bibliography, and archival work, on the other theory and interpretation. i don't have to tell you which of these two classes of work have been regarded as menial if somehow necessary. and like any system of apartheid, both groups were corrupted by it. as don mckenzie once remarked, material culture is never more grossly perceived than it is by theoreticians, whose ideas tend to remove them from base contacts with the physical objects that code and comprise material culture. but of course, as he went on to remark, the gross theoretician met his match in the myopic scholar, who gets lost in the forest by trancing on the bark of the trees.
to this day at my own university'an institution known for its commitment to serious work in textual and bibliographical studies'most of our advanced graduate students could not talk sensibly, least of all seriously or interestingly, on problems of editing and textuality and why those problems are fundamental to every kind of critical work in literary and cultural studies. i no longer ask our students in their ph.d. exams to talk about the editions they read and use, why they choose this one rather than another, what difference it would or might make. it goes without saying that these are bright and hard-working young people. nonetheless, the institutional tradition they have inherited largely set those matters at the margin of attention, and never more unfortunately so than in the last quarter of the 20th century. until that time the american research program in english studies regularly made history of the language, editing, and bibliographical studies a requirement of work. i know from my own, painful experience that these requirements were often taught in killingly mindless ways. many therefore decided that these basic disciplines had little to teach us about literature, art, and culture'either of the past or the present. as we all know, in our country these requirements were universally dropped or eviscerated between about 1965 and 1990. (in england and europe the situation is very different. highly developed philological traditions permeate their scholarship.)
when i have described our recent educational history in these terms, i have been suspected of fellow-traveling with a cadre of moralizers and promoting an instrumentalist approach to education. but remember, william bennett, denish desousa, and lynn cheney are not enemies of theory or interpretation, they are simply strict constructionists in a field where cornell west, catherine simpson, and stanley fish look for broader intellectual opportunities. seeing the educational history of the past 15 or
20 years in terms of the celebrated struggles between these groups has obscured our view of an educational emergency now grown acute with the proliferation of digital technology.
i earlier said i shouldn't be forecasting events. but here i am prepared to make a prophecy.
in the next 50 years the entirety of our inherited archive of cultural works will have to be re-edited within a network of digital storage, access, and dissemination. this system, which is already under development, is transnational and transcultural.
let's say this prophecy is true. now ask yourself these questions: 'who is carrying out this work, who will do it, who should do it?' these turn to sobering queries when we reflect on the recent history of higher education in the united states. just when we will be needing young people well-trained in the histories of textual transmission and the theory and practice of scholarly method and editing, our universities are seriously unprepared to educate such persons. electronic scholarship and editing necessarily draw their primary models from long-standing philological practices in language study, textual scholarship, and bibliography. as we know, these three core disciplines preserve but a ghostly presence in most of our ph.d. programs.
designing and executing editorial and archival projects in digital forms are now taking place and will proliferate. departments of literary study have perhaps the greatest stake in these momentous events, and yet they are'in this country'probably the least involved. the work is mostly being carried by librarians and systems engineers. many, perhaps most, of these people are smart, hardworking, and literate. their digital skills and scholarship are often outstanding. few know anything about theory of texts, and they too, like we literary and cultural types, have labored for years in intellectually underfunded conditions. it has been decades since library schools in this country required or even offered courses in the history of the book. does it shock you to learn that? we aren't shocked at our own instituted ignorance of history of the language or bibliography.
restoring intimate relations between literarians and librarians, a pressing current need, has thus been hampered from institutional developments on both sides. insofar as departments of literature participate in the work and conversations of digitized librarians, it happens through that small band of angels who continue to pursue serious editorial and bibliographical work: scholarly editors and bibliographers.
ok, then, what's the problem? our traditional departments have
managed to keep around a few old-fashioned editorial and bibliographical types. let's send them out to help with the technical jobs and hope that their'(that's our )'brains aren't completely fried by beetle-browed and positivist habits. once upon a time even they (that's we) were involved with the readerly text, right?
those contacts might perhaps prove barely sufficient were it not for another recent upheaval in the world of higher education. for it happens that between about 1965 and 1985 textual scholars began to rethink some of the most basic ideas and methods of their discipline. i chose those dates because ernest honigman published the stability of shakespeare's text in 1965, and in 1985 d. f. mckenzie delivered his famous inaugural panizzi lectures, bibliography and the sociology of texts (published 1986). so disconnected had the general scholarly community grown from its foundational subfield of textual and bibliographical studies, however, that this historic moment passed it by with little notice. the 'genetic' and 'social' editing theories and methods that emerged in those years signaled a major shift in literary and cultural scholarship. because this change overlapped with the more public emergence of what would be called literary theory'perhaps 'underlapped' is the better word'it drew scant attention to itself in that more visible orbit of literary and cultural studies. and after that came the dismal 'culture wars'.
a forthcoming publication measures the change that overtook textual scholarship at the end of the last century. in 1982 harold jenkins published his celebrated edition of hamlet in the arden shakespeare series. a lifetime's work, the book epitomized a traditional so-called eclectic approach whereby jenkins educed a single text of the play out of a careful study of the three chief documentary witnesses. soon a new arden shakespeare hamlet, edited by ann thompson and neil taylor, will replace jenkins' remarkable work. the new arden hamlet will not publish a single conflated text, it will present all three witnesses'f1 (1623), q1 (1603), and q2 (1604-5)'each in their special integrity (or lack thereof).
in may 2002 the new yorker reported this event in a substantial piece by ron rosenbaum. the article gives a good general introduction to an upheaval in textual studies that had been going on for almost 40 years, and that had been at white heat for 20. because the world of scholarship moves in a kind of slow motion, such belated awareness would not normally be cause for much notice. but at this particular historical moment, when information storage and transmission and methods of knowledge representation are calling for immediate practical attention, rosenbaum's
piece seems most interesting for what it does not talk about. force of circumstance today calls us to develop scholarly editions in digital forms. the people who used to do this work in paper forms'people like jenkins and thompson'are involved in serious controversies over how it should be done. the theory and practice of traditional textual scholarship is in a lively, not to say volatile, state of self-reflection. scholarly editing today cannot be undertaken in any medium without a disciplined engagement with editorial theory and method. scholars who think to use information technology resources, as now we must, therefore face a double difficulty. we must learn to use digital tools whose capacities are still being explored in fundamental ways even by technicians. we must also approach all the traditional questions of scholarly editing as if a transformed world stood all before us, and where to choose was fraught with uncertainty.
3.
that, ladies and gentlemen, is the context which envelops my main subject, nines (or 9s: networked infrastructure for nineteenth-century electronic scholarship). it is a three-year undertaking initiated in 2003 by myself and a group of scholars to establish an online environment for publishing peer-reviewed research in nineteenth-century british and american studies. although the resource will have significant pedagogical and classroom components, it is primarily an institutional mechanism for digitally-organized research and scholarship.
nines is conceived partly as a professional facilitator and partly as an advocacy group to protect the interests of scholars and educators. it is, as they say, results-oriented. it will liaison with interested publishing venues on behalf and in the interests of scholars and educators and the work we produce. a coordinated group of editorial boards oversees the work, which will include various kinds of content: traditional texts and documents'editions, critical works of all kinds'as well as 'born-digital' studies that relate to all aspects of nineteenth-century culture. nines is a model and working example for scholarship that takes advantage of digital resources and internet transmission. it provides scholars with access to a uniformly coded textual environment and a suite of computerized analytic and interpretive tools. a key goal of nines is to expose the rich hermeneutic potential of the electronic medium'beyond the dazzle of digital imaging and the early breakthrough of hypertext.
most important, nines is not just a committee of concerned educators who mean to discuss the problems and opportunities presented by digital technology. nines is a practical undertaking and it is already underway. here is its three year initial plan:
to establish the editorial mechanisms for soliciting, peer-reviewing, aggregating, and finally publishing born-digital scholarship and criticism in nineteenth-century british and american studies. the effort necessarily involves re-examining how traditional scholarly standards and best practices can be migrated and adapted to a digital environment.
to begin modeling a technical and institutional framework that integrates our inherited archive of paper-based materials'primary as well as secondary'with emerging forms of digital scholarship and criticism.
to develop a suite of user-friendly procedures and easily-accessible tools and applications that will help scholars and students produce interesting work in digital form. digital technology offers remarkable new possibilities for studying, analyzing, and interpreting our cultural inheritance in ways'both individual and collaborative'that have not been possible previously.
to run a series of summer fellowships for scholars who are working on it projects in nineteenth-century studies. the first workshop of twelve scholars ran for a week in the summer of 2005 at university of virginia. organized within a robust technical and scholarly environment, the workshop was designed to help the participants develop and explore their projects in the company of other scholars doing similar projects.
that is the general administrative design model of nines. under its auspices we are developing software conceived specific ally for scholars and educators working in the humanities, and in particular in literary and cultural studies. creating such applications is one of the most pressing needs we now have. unless it can provide humanists with tools and methods that overgo what we already have with book technology, why would we take any interest in it (it)?
(these tools use the special capacities of computerized systems to augment the traditional interpretive activities of the humanist scholar.)
1. a text comparison tool called juxta for comparing and collating textual similarities and differences in a given set of equivalent documents. since the critical re-editing of our inherited corpus will necessarily occupy a central focus of coming humanities scholarship, a tool of this kind is fundamental. aside from
peter robinson's collate, no such tool exists, and collate has not been widely used because it has significant limitations. juxta is entirely cross-platform and will be able to execute three basic collation and text-comparison operations: (a) collation by line or work string of poetical works (with the collation able to choose any witness as the basic reference point); (b) collation of prose documents by word string (of different sizes); (c) mining a dataset to locate and output equivalent word strings, with the output organized through a computerized analysis by degree of semantic, syntactic, or phonetic equivalence.
2. an online collaborative playspace called ivanhoe for organizing interpretive investigations of traditional humanities materials of any kind. applicable for either classroom or research use, ivanhoe's design has a double (dialectical) function: to promote the critical investigation of textual and graphical works, and to expose those investigations themselves to critical reflection and study. ivanhoe 1.0 was released this past december and is currently being used in four classes, graduate and undergraduate, at university of virginia.
3. collex. in collaboration with a project to redesign the rossetti archive, nines is developing a data model and set of tools called collex that allows users of digital resources to assemble and share virtual 'collections' and to present annotated 'exhibits' and re-arrangements of online materials. these critical rearrangements can of course bring together materials that are variously diverse'materially, formally, historically. collex is an interface for exploring complex bodies of diverse cultural materials in order to expose new networks of relations.
4. the 'patacritical demon. this is a tool for tracking and visualizing acts of critical reflection and interpretation as they are being applied in real-time to specific  works, and in particular to imaginative works like poems or stories. it is a device for addressing the following problem: how does one formalize 'exceptional' and highly subjective activities like acts of interpretation and at the same time preserve their subjective status. the demon derives its name, incidentally, from alfred jarry's proposal for a science that he called 'pataphysics, that is, 'a science of exceptions' (or 'the science of imaginary solutions').1
like ivanhoe and juxta, the 'patacritical demon outputs xml coded data. consequently, the work done with all three of these interpretive tools can be integrated with the rest of the nines-environment materials.
oh yes, one other thing. whatever happens with nines'whether that institutional event takes hold or not'these critical tools will be built. they will also be freely distributed to anyone who wants them.
conclusion
i'm a book scholar, about as traditional as you get. my work, including my theoretical work, is historicist and even philological and my orientation is decidedly humanist. 'glory to man in the highest, for man is the master of things.' that witty and impish line from swinburne is very much to my taste. men (and women) are indeed called to the mastery of things. of things precisely. of people and of life events we are and always will be participants and students, never masters. drawing that distinction'between things and people, between mastery and learning' is what it means to be'as swinburne was'a humanist.
today we have to try to master some new things. we have to learn how to make them and how to use them. to pursue that goal commits us to a demanding, and therefore humbling, adventure in knowledge. we will do this by becoming students again'a role that, as educators and humanists, i think we're especially good at. for some of us, this will be a road not taken. fair enough. but whether we choose to or not, we should all be clear about the slow train that's coming and that won't be sidetracked. 'the publishing and tenure crisis' is one certain sign of what's happening. so is the digital transformation of our research archives, the seat of
our cultural memory.
nines is a proposal to engage with these problems in specific 
and practical ways. it takes a relatively short rather than a long view' because in matters that concern us, we are always humanists, even in the short run. we know that our longest views, our totalizing conceptions, are finally only heuristic and hypothetical. but that humanist understanding is exactly why, as shelley observed, we mustn't 'let i dare not wait upon i would'. we have to get going now, we can't wait to see if there's more to learn. of course there's more to learn, that's why we must fare forth. how else will we learn what we need to know. we have to set the stage for our certain failures if we're to have any chance of measuring our measured successes. we will, as the poet observed, 'learn by going where we have to go'.
one last point is worth our reflection. capitalist entrepreneurs are already actively trying to gain control over as much information as they can. perhaps never before has knowledge been so clearly perceived as a fungible thing, as a commodity to be bought and sold. humanities scholarship has a calculable market price, and the market will work to buy low and sell high, as the dreadful examples of elsevier and kluwer have recently revealed to the science community.2
and don't imagine that our cultural heritage'what shelley called our poetry'is safe from commercial exploitation by agents that view our work'what they call 'the content' we create'as a marketable commodity. perhaps the chief virtue of a project like nines is to supply scholars with an institutional mechanism for preserving and protecting what we do.
i don't know if we will be successful in our primary objective: leveraging nines to assemble and publish an initial body of peer-reviewed online scholarship and criticism that can initiate an ongoing venue for such work, several models are imaginable that would use libraries or traditional scholarly presses as the publishing vehicles, or some combination of the two. whether or not the agents needed to make any of these models work will decide to do so is unclear. the agents'that's to say, ourselves. the matter won't become clear, one way or the other, until we undertake to design and implement a model. nines'or anything like it'can only exist in practice, not in theory.